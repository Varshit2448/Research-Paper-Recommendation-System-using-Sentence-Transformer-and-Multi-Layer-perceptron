{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ef55b8-08da-42a7-828e-1ab0f017ebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall tensorflow keras tf_keras tensorflow-intel -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ea5d2e-0ba3-4ff9-96d2-4091cc0f90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow==2.12.0 keras==2.12.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0f511-c0d6-40c8-9548-dbcd8932b364",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f49db5-7101-4895-aa02-ec1882e02af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9c1b3-7f27-43e2-9ccd-ffb9566ac049",
   "metadata": {},
   "source": [
    "# Loading CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81a3a4aa-920c-4d60-a3fe-534386e888c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.read_csv(\"arxiv_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9afc4c63-5f8c-4078-a60d-c2ed84fa31fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
       "      <td>Stereo matching is one of the widely used tech...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
       "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
       "      <td>['cs.CV', 'cs.AI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
       "      <td>Consistency training has proven to be an advan...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background-Foreground Segmentation for Interio...</td>\n",
       "      <td>To ensure safety in automated driving, the cor...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
       "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
       "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
       "4  Background-Foreground Segmentation for Interio...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Stereo matching is one of the widely used tech...   \n",
       "1  The recent advancements in artificial intellig...   \n",
       "2  In this paper, we proposed a novel mutual cons...   \n",
       "3  Consistency training has proven to be an advan...   \n",
       "4  To ensure safety in automated driving, the cor...   \n",
       "\n",
       "                         terms  \n",
       "0           ['cs.CV', 'cs.LG']  \n",
       "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
       "2           ['cs.CV', 'cs.AI']  \n",
       "3                    ['cs.CV']  \n",
       "4           ['cs.CV', 'cs.LG']  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0cb64-c935-49f7-9da7-8e44857357db",
   "metadata": {},
   "source": [
    "# Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a87024-e2a3-4700-b684-1bae999de2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51774, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05ee26aa-510d-4e2e-866b-223515f25aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "titles       0\n",
       "summaries    0\n",
       "terms        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d47cdd57-7786-4714-b9e9-f20cd2005eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12783"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db0e8517-0cd6-4bdc-9958-6788ee219cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_column = arxiv_data['terms'].apply(literal_eval)\n",
    "labels = labels_column.explode().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c99a586e-ebca-4411-8dc5-1ed9defefc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels : ['cs.CV' 'cs.LG' 'cs.AI' ... 'I.2.6; I.5.1; G.3'\n",
      " '92E10, 46M20, 94A08, 68U10, 44A12, 55R35' '92E10']\n",
      "Length of labels :  1099\n"
     ]
    }
   ],
   "source": [
    "print(\"labels :\",labels)\n",
    "print(\"Length of labels : \" , len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79b1f4eb-1f02-4cc2-bd7d-7ce6f232ebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = arxiv_data[arxiv_data['titles'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa26b60c-9eba-4da8-9bba-0ccb644067c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12802, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4cea1f6-3169-4f56-bc67-d266040c73dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605\n",
      "1038\n"
     ]
    }
   ],
   "source": [
    "print(sum(arxiv_data['terms'].value_counts() == 1))\n",
    "print(arxiv_data['terms'].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a1b567f-e38d-4603-8823-c85d00e84cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12197, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data_filtered = arxiv_data.groupby('terms').filter(lambda x : len(x) > 1)\n",
    "arxiv_data_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc9235c3-1a74-4ac5-9006-b6afdaae031a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['stat.ML', 'cs.CV']), list(['cs.CV', 'cs.AI']),\n",
       "       list(['cs.CV'])], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data_filtered['terms'] = arxiv_data_filtered['terms'].apply(lambda x: literal_eval(x))\n",
    "arxiv_data_filtered['terms'].values[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537cfb61-6e72-4f09-99cc-2d654ee38638",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1dad179-3a6e-4579-9498-4785ca97dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df , test_df = train_test_split(arxiv_data_filtered , test_size = 0.1 , stratify = arxiv_data_filtered['terms'].values ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8465efe-cae9-4774-be53-f41af8eb366d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10977, 3), (1220, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape , test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8b1db4b-8818-4e4d-9ebe-a17a24004f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = test_df.sample(frac = 0.5)\n",
    "test_df.drop(val_df.index , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d239883e-a31f-48b4-8ef1-6ba1e46e35f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((610, 3), (610, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape , test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e295e8e-25be-45d7-aece-149a1911f2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22157</th>\n",
       "      <td>Higher-Order Attribute-Enhancing Heterogeneous...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "      <td>[cs.LG, cs.SI]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22315</th>\n",
       "      <td>Entity Context Graph: Learning Entity Represen...</td>\n",
       "      <td>Knowledge is captured in the form of entities ...</td>\n",
       "      <td>[cs.LG, cs.CL, cs.IR]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35369</th>\n",
       "      <td>Deep Generative Models with Learnable Knowledg...</td>\n",
       "      <td>The broad set of deep generative models (DGMs)...</td>\n",
       "      <td>[cs.LG, cs.CL, cs.CV, stat.ML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38670</th>\n",
       "      <td>Spectral Temporal Graph Neural Network for Tra...</td>\n",
       "      <td>An effective understanding of the contextual e...</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG, cs.RO]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23646</th>\n",
       "      <td>About Graph Degeneracy, Representation Learnin...</td>\n",
       "      <td>Graphs or networks are a very convenient way t...</td>\n",
       "      <td>[cs.LG, stat.ML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45085</th>\n",
       "      <td>Unsupervised Image Noise Modeling with Self-Co...</td>\n",
       "      <td>Noise modeling lies in the heart of many image...</td>\n",
       "      <td>[cs.CV, eess.IV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38732</th>\n",
       "      <td>Towards Efficient Cross-Modal Visual Textual R...</td>\n",
       "      <td>Cross-modal retrieval is an important function...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44452</th>\n",
       "      <td>Deep learning for time series classification</td>\n",
       "      <td>Time series analysis is a field of data scienc...</td>\n",
       "      <td>[cs.LG, cs.AI, stat.ML]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47776</th>\n",
       "      <td>Free-Lunch Saliency via Attention in Atari Agents</td>\n",
       "      <td>We propose a new approach to visualize salienc...</td>\n",
       "      <td>[cs.LG, cs.AI, cs.CV]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41816</th>\n",
       "      <td>Action Modifiers: Learning from Adverbs in Ins...</td>\n",
       "      <td>We present a method to learn a representation ...</td>\n",
       "      <td>[cs.CV]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10977 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  titles  \\\n",
       "22157  Higher-Order Attribute-Enhancing Heterogeneous...   \n",
       "22315  Entity Context Graph: Learning Entity Represen...   \n",
       "35369  Deep Generative Models with Learnable Knowledg...   \n",
       "38670  Spectral Temporal Graph Neural Network for Tra...   \n",
       "23646  About Graph Degeneracy, Representation Learnin...   \n",
       "...                                                  ...   \n",
       "45085  Unsupervised Image Noise Modeling with Self-Co...   \n",
       "38732  Towards Efficient Cross-Modal Visual Textual R...   \n",
       "44452       Deep learning for time series classification   \n",
       "47776  Free-Lunch Saliency via Attention in Atari Agents   \n",
       "41816  Action Modifiers: Learning from Adverbs in Ins...   \n",
       "\n",
       "                                               summaries  \\\n",
       "22157  Graph neural networks (GNNs) have been widely ...   \n",
       "22315  Knowledge is captured in the form of entities ...   \n",
       "35369  The broad set of deep generative models (DGMs)...   \n",
       "38670  An effective understanding of the contextual e...   \n",
       "23646  Graphs or networks are a very convenient way t...   \n",
       "...                                                  ...   \n",
       "45085  Noise modeling lies in the heart of many image...   \n",
       "38732  Cross-modal retrieval is an important function...   \n",
       "44452  Time series analysis is a field of data scienc...   \n",
       "47776  We propose a new approach to visualize salienc...   \n",
       "41816  We present a method to learn a representation ...   \n",
       "\n",
       "                                terms  \n",
       "22157                  [cs.LG, cs.SI]  \n",
       "22315           [cs.LG, cs.CL, cs.IR]  \n",
       "35369  [cs.LG, cs.CL, cs.CV, stat.ML]  \n",
       "38670    [cs.CV, cs.AI, cs.LG, cs.RO]  \n",
       "23646                [cs.LG, stat.ML]  \n",
       "...                               ...  \n",
       "45085                [cs.CV, eess.IV]  \n",
       "38732                         [cs.CV]  \n",
       "44452         [cs.LG, cs.AI, stat.ML]  \n",
       "47776           [cs.LG, cs.AI, cs.CV]  \n",
       "41816                         [cs.CV]  \n",
       "\n",
       "[10977 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74ea0c4c-42ad-4313-b771-7d6d7bcd2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = tf.ragged.constant(train_df['terms'])\n",
    "lookup = tf.keras.layers.StringLookup(output_mode = 'multi_hot')\n",
    "lookup.adapt(terms)\n",
    "vocab = lookup.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "deb90e6c-12a0-47bb-b4b8-ff8408785d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cs.LG', 'cs.SI']\n",
      "tf.Tensor(\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(1, 159), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample_label = train_df['terms'].iloc[0]\n",
    "print(sample_label)\n",
    "label_binarized = lookup([sample_label])\n",
    "print(label_binarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a92e4b0-ac1b-4915-8851-118042a599ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset element spec: (TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 159), dtype=tf.float32, name=None))\n",
      "Validation dataset element spec: (TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 159), dtype=tf.float32, name=None))\n",
      "Test dataset element spec: (TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 159), dtype=tf.float32, name=None))\n"
     ]
    }
   ],
   "source": [
    "max_seqlen = 150\n",
    "batch_size = 120\n",
    "padding_token = \"<pad>\"\n",
    "auto = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(dataframe, is_train=True):\n",
    "    # Convert terms column to a tensor that can be passed to the lookup layer\n",
    "    terms_ragged = tf.ragged.constant(dataframe['terms'].tolist())\n",
    "    \n",
    "    # Apply the lookup transformation to each term\n",
    "    label_binary = lookup(terms_ragged).numpy()  # Returns a multi-hot encoded matrix\n",
    "    \n",
    "    # Ensure the shapes align for abstracts and label_binary\n",
    "    assert len(dataframe['summaries'].values) == label_binary.shape[0], \"Mismatch in lengths of abstracts and labels\"\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dataframe['summaries'].values, label_binary))\n",
    "    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n",
    "\n",
    "    return dataset.batch(batch_size)\n",
    "    \n",
    "\n",
    "    \n",
    "train_dataset = make_dataset(train_df , is_train = True)\n",
    "val_dataset = make_dataset(val_df , is_train = False)\n",
    "test_dataset = make_dataset(train_df , is_train = False)\n",
    "\n",
    "\n",
    "print(\"Train dataset element spec:\", train_dataset.element_spec)\n",
    "print(\"Validation dataset element spec:\", val_dataset.element_spec)\n",
    "print(\"Test dataset element spec:\", test_dataset.element_spec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a010f209-3e22-467c-bc35-03b277108adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Modern data acquisition routinely produce massive amounts of event sequence\\ndata in various domains, such as social media, healthcare, and financial\\nmarkets. These data often exhibit complicated short-term and long-term temporal\\ndependencies. However, most of the existing recurrent neural network based\\npoint process models fail to capture such dependencies, and yield unreliable\\nprediction performance. To address this issue, we propose a Transformer Hawkes\\nProcess (THP) model, which leverages the self-attention mechanism to capture\\nlong-term dependencies and meanwhile enjoys computational efficiency. Numerical\\nexperiments on various datasets show that THP outperforms existing models in\\nterms of both likelihood and event prediction accuracy by a notable margin.\\nMoreover, THP is quite general and can incorporate additional structural\\nknowledge. We provide a concrete example, where THP achieves improved\\nprediction performance for learning multiple point processes when incorporating\\ntheir relational information.', shape=(), dtype=string)\n",
      "['cs.LG' 'stat.ML']\n",
      "tf.Tensor(b'The strict security requirements placed on medical records by various privacy\\nregulations become major obstacles in the age of big data. To ensure efficient\\nmachine learning as a service schemes while protecting data confidentiality, in\\nthis work, we propose blind UNET (BUNET), a secure protocol that implements\\nprivacy-preserving medical image segmentation based on the UNET architecture.\\nIn BUNET, we efficiently utilize cryptographic primitives such as homomorphic\\nencryption and garbled circuits (GC) to design a complete secure protocol for\\nthe UNET neural architecture. In addition, we perform extensive architectural\\nsearch in reducing the computational bottleneck of GC-based secure activation\\nprotocols with high-dimensional input data. In the experiment, we thoroughly\\nexamine the parameter space of our protocol, and show that we can achieve up to\\n14x inference time reduction compared to the-state-of-the-art secure inference\\ntechnique on a baseline architecture with negligible accuracy degradation.', shape=(), dtype=string)\n",
      "['cs.CV' 'cs.CR']\n",
      "tf.Tensor(b'Dialog State Tracking (DST) is one of the most crucial modules for\\ngoal-oriented dialogue systems. In this paper, we introduce FastSGT (Fast\\nSchema Guided Tracker), a fast and robust BERT-based model for state tracking\\nin goal-oriented dialogue systems. The proposed model is designed for the\\nSchema-Guided Dialogue (SGD) dataset which contains natural language\\ndescriptions for all the entities including user intents, services, and slots.\\nThe model incorporates two carry-over procedures for handling the extraction of\\nthe values not explicitly mentioned in the current user utterance. It also uses\\nmulti-head attention projections in some of the decoders to have a better\\nmodelling of the encoder outputs. In the conducted experiments we compared\\nFastSGT to the baseline model for the SGD dataset. Our model keeps the\\nefficiency in terms of computational and memory consumption while improving the\\naccuracy significantly. Additionally, we present ablation studies measuring the\\nimpact of different parts of the model on its performance. We also show the\\neffectiveness of data augmentation for improving the accuracy without\\nincreasing the amount of computational resources.', shape=(), dtype=string)\n",
      "['cs.LG' 'stat.ML']\n",
      "tf.Tensor(b'Graph neural networks (GNNs) have achieved great success in many graph-based\\ntasks. Much work is dedicated to empowering GNNs with the adaptive locality\\nability, which enables measuring the importance of neighboring nodes to the\\ntarget node by a node-specific mechanism. However, the current node-specific\\nmechanisms are deficient in distinguishing the importance of nodes in the\\ntopology structure. We believe that the structural importance of neighboring\\nnodes is closely related to their importance in aggregation. In this paper, we\\nintroduce discrete graph curvature (the Ricci curvature) to quantify the\\nstrength of structural connection of pairwise nodes. And we propose Curvature\\nGraph Neural Network (CGNN), which effectively improves the adaptive locality\\nability of GNNs by leveraging the structural property of graph curvature. To\\nimprove the adaptability of curvature to various datasets, we explicitly\\ntransform curvature into the weights of neighboring nodes by the necessary\\nNegative Curvature Processing Module and Curvature Normalization Module. Then,\\nwe conduct numerous experiments on various synthetic datasets and real-world\\ndatasets. The experimental results on synthetic datasets show that CGNN\\neffectively exploits the topology structure information, and the performance is\\nimproved significantly. CGNN outperforms the baselines on 5 dense node\\nclassification benchmark datasets. This study deepens the understanding of how\\nto utilize advanced topology information and assign the importance of\\nneighboring nodes from the perspective of graph curvature and encourages us to\\nbridge the gap between graph theory and neural networks.', shape=(), dtype=string)\n",
      "['cs.LG' 'cs.AI']\n",
      "tf.Tensor(b\"Although spatial information of images usually enhance the robustness of the\\nFuzzy C-Means (FCM) algorithm, it greatly increases the computational costs for\\nimage segmentation. To achieve a sound trade-off between the segmentation\\nperformance and the speed of clustering, we come up with a Kullback-Leibler\\n(KL) divergence-based FCM algorithm by incorporating a tight wavelet frame\\ntransform and a morphological reconstruction operation. To enhance FCM's\\nrobustness, an observed image is first filtered by using the morphological\\nreconstruction. A tight wavelet frame system is employed to decompose the\\nobserved and filtered images so as to form their feature sets. Considering\\nthese feature sets as data of clustering, an modified FCM algorithm is\\nproposed, which introduces a KL divergence term in the partition matrix into\\nits objective function. The KL divergence term aims to make membership degrees\\nof each image pixel closer to those of its neighbors, which brings that the\\nmembership partition becomes more suitable and the parameter setting of FCM\\nbecomes simplified. On the basis of the obtained partition matrix and\\nprototypes, the segmented feature set is reconstructed by minimizing the\\ninverse process of the modified objective function. To modify abnormal features\\nproduced in the reconstruction process, each reconstructed feature is\\nreassigned to the closest prototype. As a result, the segmentation accuracy of\\nKL divergence-based FCM is further improved. What's more, the segmented image\\nis reconstructed by using a tight wavelet frame reconstruction operation.\\nFinally, supporting experiments coping with synthetic, medical and color images\\nare reported. Experimental results exhibit that the proposed algorithm works\\nwell and comes with better segmentation performance than other comparative\\nalgorithms. Moreover, the proposed algorithm requires less time than most of\\nthe FCM-related algorithms.\", shape=(), dtype=string)\n",
      "['cs.CV' 'I.4.6' '62H30']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def invert_multi_hot(encoded_labels):\n",
    "    hot_indeces = np.argwhere(encoded_labels == 1.0)[... , 0]\n",
    "    return np.take(vocab , hot_indeces)\n",
    "\n",
    "text_batch , label_batch = next(iter(train_dataset))\n",
    "for i , text in enumerate(text_batch[:5]):\n",
    "    label = label_batch[i].numpy()[None , ...]\n",
    "    print(text)\n",
    "    print(invert_multi_hot(label[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75981088-55cd-4a1c-9659-fda8be70e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "train_df['summaries'].str.lower().str.split().apply(vocabulary.update)\n",
    "vocabulary_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be459187-20aa-4aec-8d97-523107126cd0",
   "metadata": {},
   "source": [
    "# Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78e4d8d6-d31a-4b5b-ad9b-f96f05bf11c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer = layers.TextVectorization(max_tokens = vocabulary_size , ngrams = 2 , output_mode = 'tf_idf')\n",
    "text_vectorizer.adapt(train_dataset.map(lambda text , label : text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31a88ebc-a4c8-4be2-9718-f307b18194a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in the training dataset: 92\n"
     ]
    }
   ],
   "source": [
    "num_records = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "print(\"Number of records in the training dataset:\", num_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfc0203b-a33e-48d1-858b-beeee91608a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Label sample type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Text sample: [b'The richness in the content of various information networks such as social\\nnetworks and communication networks provides the unprecedented potential for\\nlearning high-quality expressive representations without external supervision.\\nThis paper investigates how to preserve and extract the abundant information\\nfrom graph-structured data into embedding space in an unsupervised manner. To\\nthis end, we propose a novel concept, Graphical Mutual Information (GMI), to\\nmeasure the correlation between input graphs and high-level hidden\\nrepresentations. GMI generalizes the idea of conventional mutual information\\ncomputations from vector space to the graph domain where measuring mutual\\ninformation from two aspects of node features and topological structure is\\nindispensable. GMI exhibits several benefits: First, it is invariant to the\\nisomorphic transformation of input graphs---an inevitable constraint in many\\nexisting graph representation learning algorithms; Besides, it can be\\nefficiently estimated and maximized by current mutual information estimation\\nmethods such as MINE; Finally, our theoretical analysis confirms its\\ncorrectness and rationality. With the aid of GMI, we develop an unsupervised\\nlearning model trained by maximizing GMI between the input and output of a\\ngraph neural encoder. Considerable experiments on transductive as well as\\ninductive node classification and link prediction demonstrate that our method\\noutperforms state-of-the-art unsupervised counterparts, and even sometimes\\nexceeds the performance of supervised ones.'\n",
      " b\"Knowledge graphs (KGs) are of great importance to many real world\\napplications, but they generally suffer from incomplete information in the form\\nof missing relations between entities. Knowledge graph completion (also known\\nas relation prediction) is the task of inferring missing facts given existing\\nones. Most of the existing work is proposed by maximizing the likelihood of\\nobserved instance-level triples. Not much attention, however, is paid to the\\nontological information, such as type information of entities and relations. In\\nthis work, we propose a type-augmented relation prediction (TaRP) method, where\\nwe apply both the type information and instance-level information for relation\\nprediction. In particular, type information and instance-level information are\\nencoded as prior probabilities and likelihoods of relations respectively, and\\nare combined by following Bayes' rule. Our proposed TaRP method achieves\\nsignificantly better performance than state-of-the-art methods on four\\nbenchmark datasets: FB15K, FB15K-237, YAGO26K-906, and DB111K-174. In addition,\\nwe show that TaRP achieves significantly improved data efficiency. More\\nimportantly, the type information extracted from a specific dataset can\\ngeneralize well to other datasets through the proposed TaRP model.\"\n",
      " b'Despite all the challenges and limitations, vision-based vehicle speed\\ndetection is gaining research interest due to its great potential benefits such\\nas cost reduction, and enhanced additional functions. As stated in a recent\\nsurvey [1], the use of learning-based approaches to address this problem is\\nstill in its infancy. One of the main difficulties is the need for a large\\namount of data, which must contain the input sequences and, more importantly,\\nthe output values corresponding to the actual speed of the vehicles. Data\\ncollection in this context requires a complex and costly setup to capture the\\nimages from the camera synchronized with a high precision speed sensor to\\ngenerate the ground truth speed values. In this paper we explore, for the first\\ntime, the use of synthetic images generated from a driving simulator (e.g.,\\nCARLA) to address vehicle speed detection using a learning-based approach. We\\nsimulate a virtual camera placed over a stretch of road, and generate thousands\\nof images with variability corresponding to multiple speeds, different vehicle\\ntypes and colors, and lighting and weather conditions. Two different approaches\\nto map the sequence of images to an output speed (regression) are studied,\\nincluding CNN-GRU and 3D-CNN. We present preliminary results that support the\\nhigh potential of this approach to address vehicle speed detection.'\n",
      " b\"Environment perception is crucial for autonomous vehicle (AV) safety. Most\\nexisting AV perception algorithms have not studied the surrounding environment\\ncomplexity and failed to include the environment complexity parameter. This\\npaper proposes a novel attention-based neural network model to predict the\\ncomplexity level of the surrounding driving environment. The proposed model\\ntakes naturalistic driving videos and corresponding vehicle dynamics parameters\\nas input. It consists of a Yolo-v3 object detection algorithm, a heat map\\ngeneration algorithm, CNN-based feature extractors, and attention-based feature\\nextractors for both video and time-series vehicle dynamics data inputs to\\nextract features. The output from the proposed algorithm is a surrounding\\nenvironment complexity parameter. The Berkeley DeepDrive dataset (BDD Dataset)\\nand subjectively labeled surrounding environment complexity levels are used for\\nmodel training and validation to evaluate the algorithm. The proposed\\nattention-based network achieves 91.22% average classification accuracy to\\nclassify the surrounding environment complexity. It proves that the environment\\ncomplexity level can be accurately predicted and applied for future AVs'\\nenvironment perception studies.\"\n",
      " b'Bottom-up and top-down visual cues are two types of information that helps\\nthe visual saliency models. These salient cues can be from spatial\\ndistributions of the features (space-based saliency) or contextual /\\ntask-dependent features (object based saliency). Saliency models generally\\nincorporate salient cues either in bottom-up or top-down norm separately. In\\nthis work, we combine bottom-up and top-down cues from both space and object\\nbased salient features on RGB-D data. In addition, we also investigated the\\nability of various pre-trained convolutional neural networks for extracting\\ntop-down saliency on color images based on the object dependent feature\\nactivation. We demonstrate that combining salient features from color and dept\\nthrough bottom-up and top-down methods gives significant improvement on the\\nsalient object detection with space based and object based salient cues. RGB-D\\nsaliency integration framework yields promising results compared with the\\nseveral state-of-the-art-models.'\n",
      " b'A table arranging data in rows and columns is a very effective data\\nstructure, which has been widely used in business and scientific research.\\nConsidering large-scale tabular data in online and offline documents, automatic\\ntable recognition has attracted increasing attention from the document analysis\\ncommunity. Though human can easily understand the structure of tables, it\\nremains a challenge for machines to understand that, especially due to a\\nvariety of different table layouts and styles. Existing methods usually model a\\ntable as either the markup sequence or the adjacency matrix between different\\ntable cells, failing to address the importance of the logical location of table\\ncells, e.g., a cell is located in the first row and the second column of the\\ntable. In this paper, we reformulate the problem of table structure recognition\\nas the table graph reconstruction, and propose an end-to-end trainable table\\ngraph reconstruction network (TGRNet) for table structure recognition.\\nSpecifically, the proposed method has two main branches, a cell detection\\nbranch and a cell logical location branch, to jointly predict the spatial\\nlocation and the logical location of different cells. Experimental results on\\nthree popular table recognition datasets and a new dataset with table graph\\nannotations (TableGraph-350K) demonstrate the effectiveness of the proposed\\nTGRNet for table structure recognition. Code and annotations will be made\\npublicly available.'\n",
      " b'Molecular graph generation is a fundamental problem for drug discovery and\\nhas been attracting growing attention. The problem is challenging since it\\nrequires not only generating chemically valid molecular structures but also\\noptimizing their chemical properties in the meantime. Inspired by the recent\\nprogress in deep generative models, in this paper we propose a flow-based\\nautoregressive model for graph generation called GraphAF. GraphAF combines the\\nadvantages of both autoregressive and flow-based approaches and enjoys: (1)\\nhigh model flexibility for data density estimation; (2) efficient parallel\\ncomputation for training; (3) an iterative sampling process, which allows\\nleveraging chemical domain knowledge for valency checking. Experimental results\\nshow that GraphAF is able to generate 68% chemically valid molecules even\\nwithout chemical knowledge rules and 100% valid molecules with chemical rules.\\nThe training process of GraphAF is two times faster than the existing\\nstate-of-the-art approach GCPN. After fine-tuning the model for goal-directed\\nproperty optimization with reinforcement learning, GraphAF achieves\\nstate-of-the-art performance on both chemical property optimization and\\nconstrained property optimization.'\n",
      " b'As digital medical imaging becomes more prevalent and archives increase in\\nsize, representation learning exposes an interesting opportunity for enhanced\\nmedical decision support systems. On the other hand, medical imaging data is\\noften scarce and short on annotations. In this paper, we present an assessment\\nof unsupervised feature learning approaches for images in the biomedical\\nliterature, which can be applied to automatic biomedical concept detection. Six\\nunsupervised representation learning methods were built, including traditional\\nbags of visual words, autoencoders, and generative adversarial networks. Each\\nmodel was trained, and their respective feature space evaluated using images\\nfrom the ImageCLEF 2017 concept detection task. We conclude that it is possible\\nto obtain more powerful representations with modern deep learning approaches,\\nin contrast with previously popular computer vision methods. Although\\ngenerative adversarial networks can provide good results, they are harder to\\nsucceed in highly varied data sets. The possibility of semi-supervised\\nlearning, as well as their use in medical information retrieval problems, are\\nthe next steps to be strongly considered.'\n",
      " b'State-of-the-art results on image recognition tasks are achieved using\\nover-parameterized learning algorithms that (nearly) perfectly fit the training\\nset and are known to fit well even random labels. This tendency to memorize the\\nlabels of the training data is not explained by existing theoretical analyses.\\nMemorization of the training data also presents significant privacy risks when\\nthe training data contains sensitive personal information and thus it is\\nimportant to understand whether such memorization is necessary for accurate\\nlearning.\\n  We provide the first conceptual explanation and a theoretical model for this\\nphenomenon. Specifically, we demonstrate that for natural data distributions\\nmemorization of labels is necessary for achieving close-to-optimal\\ngeneralization error. Crucially, even labels of outliers and noisy labels need\\nto be memorized. The model is motivated and supported by the results of several\\nrecent empirical works. In our model, data is sampled from a mixture of\\nsubpopulations and our results show that memorization is necessary whenever the\\ndistribution of subpopulation frequencies is long-tailed. Image and text data\\nis known to be long-tailed and therefore our results establish a formal link\\nbetween these empirical phenomena. Our results allow to quantify the cost of\\nlimiting memorization in learning and explain the disparate effects that\\nprivacy and model compression have on different subgroups.'\n",
      " b\"Resource allocation and transceivers in wireless networks are usually\\ndesigned by solving optimization problems subject to specific constraints,\\nwhich can be formulated as variable or functional optimization. If the\\nobjective and constraint functions of a variable optimization problem can be\\nderived, standard numerical algorithms can be applied for finding the optimal\\nsolution, which however incur high computational cost when the dimension of the\\nvariable is high. To reduce the on-line computational complexity, learning the\\noptimal solution as a function of the environment's status by deep neural\\nnetworks (DNNs) is an effective approach. DNNs can be trained under the\\nsupervision of optimal solutions, which however, is not applicable to the\\nscenarios without models or for functional optimization where the optimal\\nsolutions are hard to obtain. If the objective and constraint functions are\\nunavailable, reinforcement learning can be applied to find the solution of a\\nfunctional optimization problem, which is however not tailored to optimization\\nproblems in wireless networks. In this article, we introduce unsupervised and\\nreinforced-unsupervised learning frameworks for solving both variable and\\nfunctional optimization problems without the supervision of the optimal\\nsolutions. When the mathematical model of the environment is completely known\\nand the distribution of environment's status is known or unknown, we can invoke\\nunsupervised learning algorithm. When the mathematical model of the environment\\nis incomplete, we introduce reinforced-unsupervised learning algorithms that\\nlearn the model by interacting with the environment. Our simulation results\\nconfirm the applicability of these learning frameworks by taking a user\\nassociation problem as an example.\"\n",
      " b\"Thanks to their remarkable generative capabilities, GANs have gained great\\npopularity, and are used abundantly in state-of-the-art methods and\\napplications. In a GAN based model, a discriminator is trained to learn the\\nreal data distribution. To date, it has been used only for training purposes,\\nwhere it's utilized to train the generator to provide real-looking outputs. In\\nthis paper we propose a novel method that makes an explicit use of the\\ndiscriminator in test-time, in a feedback manner in order to improve the\\ngenerator results. To the best of our knowledge it is the first time a\\ndiscriminator is involved in test-time. We claim that the discriminator holds\\nsignificant information on the real data distribution, that could be useful for\\ntest-time as well, a potential that has not been explored before.\\n  The approach we propose does not alter the conventional training stage. At\\ntest-time, however, it transfers the output from the generator into the\\ndiscriminator, and uses feedback modules (convolutional blocks) to translate\\nthe features of the discriminator layers into corrections to the features of\\nthe generator layers, which are used eventually to get a better generator\\nresult. Our method can contribute to both conditional and unconditional GANs.\\nAs demonstrated by our experiments, it can improve the results of\\nstate-of-the-art networks for super-resolution, and image generation.\"\n",
      " b\"It is common but challenging to address high-resolution image blending in the\\nautomatic photo editing application. In this paper, we would like to focus on\\nsolving the problem of high-resolution image blending, where the composite\\nimages are provided. We propose a framework called Gaussian-Poisson Generative\\nAdversarial Network (GP-GAN) to leverage the strengths of the classical\\ngradient-based approach and Generative Adversarial Networks. To the best of our\\nknowledge, it's the first work that explores the capability of GANs in\\nhigh-resolution image blending task. Concretely, we propose Gaussian-Poisson\\nEquation to formulate the high-resolution image blending problem, which is a\\njoint optimization constrained by the gradient and color information. Inspired\\nby the prior works, we obtain gradient information via applying gradient\\nfilters. To generate the color information, we propose a Blending GAN to learn\\nthe mapping between the composite images and the well-blended ones. Compared to\\nthe alternative methods, our approach can deliver high-resolution, realistic\\nimages with fewer bleedings and unpleasant artifacts. Experiments confirm that\\nour approach achieves the state-of-the-art performance on Transient Attributes\\ndataset. A user study on Amazon Mechanical Turk finds that the majority of\\nworkers are in favor of the proposed method.\"\n",
      " b'Conditional image modeling based on textual descriptions is a relatively new\\ndomain in unsupervised learning. Previous approaches use a latent variable\\nmodel and generative adversarial networks. While the formers are approximated\\nby using variational auto-encoders and rely on the intractable inference that\\ncan hamper their performance, the latter is unstable to train due to Nash\\nequilibrium based objective function. We develop a tractable and stable\\ncaption-based image generation model. The model uses an attention-based encoder\\nto learn word-to-pixel dependencies. A conditional autoregressive based decoder\\nis used for learning pixel-to-pixel dependencies and generating images.\\nExperimentations are performed on Microsoft COCO, and MNIST-with-captions\\ndatasets and performance is evaluated by using the Structural Similarity Index.\\nResults show that the proposed model performs better than contemporary\\napproaches and generate better quality images. Keywords: Generative image\\nmodeling, autoregressive image modeling, caption-based image generation, neural\\nattention, recurrent neural networks.'\n",
      " b'This paper proposes \\\\textit{layer fusion} - a model compression technique\\nthat discovers which weights to combine and then fuses weights of similar\\nfully-connected, convolutional and attention layers. Layer fusion can\\nsignificantly reduce the number of layers of the original network with little\\nadditional computation overhead, while maintaining competitive performance.\\nFrom experiments on CIFAR-10, we find that various deep convolution neural\\nnetworks can remain within 2\\\\% accuracy points of the original networks up to a\\ncompression ratio of 3.33 when iteratively retrained with layer fusion. For\\nexperiments on the WikiText-2 language modelling dataset where pretrained\\ntransformer models are used, we achieve compression that leads to a network\\nthat is 20\\\\% of its original size while being within 5 perplexity points of the\\noriginal network. We also find that other well-established compression\\ntechniques can achieve competitive performance when compared to their original\\nnetworks given a sufficient number of retraining steps. Generally, we observe a\\nclear inflection point in performance as the amount of compression increases,\\nsuggesting a bound on the amount of compression that can be achieved before an\\nexponential degradation in performance.'\n",
      " b'Learning information-rich and generalizable representations effectively from\\nunlabeled multivariate cardiac signals to identify abnormal heart rhythms\\n(cardiac arrhythmias) is valuable in real-world clinical settings but often\\nchallenging due to its complex temporal dynamics. Cardiac arrhythmias can vary\\nsignificantly in temporal patterns even for the same patient ($i.e.$, intra\\nsubject difference). Meanwhile, the same type of cardiac arrhythmia can show\\ndifferent temporal patterns among different patients due to different cardiac\\nstructures ($i.e.$, inter subject difference). In this paper, we address the\\nchallenges by proposing an Intra-inter Subject self-supervised Learning (ISL)\\nmodel that is customized for multivariate cardiac signals. Our proposed ISL\\nmodel integrates medical knowledge into self-supervision to effectively learn\\nfrom intra-inter subject differences. In intra subject self-supervision, ISL\\nmodel first extracts heartbeat-level features from each subject using a\\nchannel-wise attentional CNN-RNN encoder. Then a stationarity test module is\\nemployed to capture the temporal dependencies between heartbeats. In inter\\nsubject self-supervision, we design a set of data augmentations according to\\nthe clinical characteristics of cardiac signals and perform contrastive\\nlearning among subjects to learn distinctive representations for various types\\nof patients. Extensive experiments on three real-world datasets were conducted.\\nIn a semi-supervised transfer learning scenario, our pre-trained ISL model\\nleads about 10% improvement over supervised training when only 1% labeled data\\nis available, suggesting strong generalizability and robustness of the model.'\n",
      " b'Fine-grained visual categorization is to recognize hundreds of subcategories\\nbelonging to the same basic-level category, which is a highly challenging task\\ndue to the quite subtle and local visual distinctions among similar\\nsubcategories. Most existing methods generally learn part detectors to discover\\ndiscriminative regions for better categorization performance. However, not all\\nparts are beneficial and indispensable for visual categorization, and the\\nsetting of part detector number heavily relies on prior knowledge as well as\\nexperimental validation. As is known to all, when we describe the object of an\\nimage via textual descriptions, we mainly focus on the pivotal characteristics,\\nand rarely pay attention to common characteristics as well as the background\\nareas. This is an involuntary transfer from human visual attention to textual\\nattention, which leads to the fact that textual attention tells us how many and\\nwhich parts are discriminative and significant to categorization. So textual\\nattention could help us to discover visual attention in image. Inspired by\\nthis, we propose a fine-grained visual-textual representation learning (VTRL)\\napproach, and its main contributions are: (1) Fine-grained visual-textual\\npattern mining devotes to discovering discriminative visual-textual pairwise\\ninformation for boosting categorization performance through jointly modeling\\nvision and text with generative adversarial networks (GANs), which\\nautomatically and adaptively discovers discriminative parts. (2) Visual-textual\\nrepresentation learning jointly combines visual and textual information, which\\npreserves the intra-modality and inter-modality information to generate\\ncomplementary fine-grained representation, as well as further improves\\ncategorization performance.'\n",
      " b'Region-based methods have proven necessary for improving segmentation\\naccuracy of neuronal structures in electron microscopy (EM) images. Most\\nregion-based segmentation methods use a scoring function to determine region\\nmerging. Such functions are usually learned with supervised algorithms that\\ndemand considerable ground truth data, which are costly to collect. We propose\\na semi-supervised approach that reduces this demand. Based on a merge tree\\nstructure, we develop a differentiable unsupervised loss term that enforces\\nconsistent predictions from the learned function. We then propose a Bayesian\\nmodel that combines the supervised and the unsupervised information for\\nprobabilistic learning. The experimental results on three EM data sets\\ndemonstrate that by using a subset of only 3% to 7% of the entire ground truth\\ndata, our approach consistently performs close to the state-of-the-art\\nsupervised method with the full labeled data set, and significantly outperforms\\nthe supervised method with the same labeled subset.'\n",
      " b\"Flat surfaces captured by 3D point clouds are often used for localization,\\nmapping, and modeling. Dense point cloud processing has high computation and\\nmemory costs making low-dimensional representations of flat surfaces such as\\npolygons desirable. We present Polylidar3D, a non-convex polygon extraction\\nalgorithm which takes as input unorganized 3D point clouds (e.g., LiDAR data),\\norganized point clouds (e.g., range images), or user-provided meshes.\\nNon-convex polygons represent flat surfaces in an environment with interior\\ncutouts representing obstacles or holes. The Polylidar3D front-end transforms\\ninput data into a half-edge triangular mesh. This representation provides a\\ncommon level of input data abstraction for subsequent back-end processing. The\\nPolylidar3D back-end is composed of four core algorithms: mesh smoothing,\\ndominant plane normal estimation, planar segment extraction, and finally\\npolygon extraction. Polylidar3D is shown to be quite fast, making use of CPU\\nmulti-threading and GPU acceleration when available. We demonstrate\\nPolylidar3D's versatility and speed with real-world datasets including aerial\\nLiDAR point clouds for rooftop mapping, autonomous driving LiDAR point clouds\\nfor road surface detection, and RGBD cameras for indoor floor/wall detection.\\nWe also evaluate Polylidar3D on a challenging planar segmentation benchmark\\ndataset. Results consistently show excellent speed and accuracy.\"\n",
      " b'This paper is a contribution towards interpretability of the deep learning\\nmodels in different applications of time-series. We propose a temporal\\nattention layer that is capable of selecting the relevant information to\\nperform various tasks, including data completion, key-frame detection and\\nclassification. The method uses the whole input sequence to calculate an\\nattention value for each time step. This results in more focused attention\\nvalues and more plausible visualisation than previous methods. We apply the\\nproposed method to three different tasks. Experimental results show that the\\nproposed network produces comparable results to a state of the art. In\\naddition, the network provides better interpretability of the decision, that\\nis, it generates more significant attention weight to related frames compared\\nto similar techniques attempted in the past.'\n",
      " b\"Single domain generalization is a challenging case of model generalization,\\nwhere the models are trained on a single domain and tested on other unseen\\ndomains. A promising solution is to learn cross-domain invariant\\nrepresentations by expanding the coverage of the training domain. These methods\\nhave limited generalization performance gains in practical applications due to\\nthe lack of appropriate safety and effectiveness constraints. In this paper, we\\npropose a novel learning framework called progressive domain expansion network\\n(PDEN) for single domain generalization. The domain expansion subnetwork and\\nrepresentation learning subnetwork in PDEN mutually benefit from each other by\\njoint learning. For the domain expansion subnetwork, multiple domains are\\nprogressively generated in order to simulate various photometric and geometric\\ntransforms in unseen domains. A series of strategies are introduced to\\nguarantee the safety and effectiveness of the expanded domains. For the domain\\ninvariant representation learning subnetwork, contrastive learning is\\nintroduced to learn the domain invariant representation in which each class is\\nwell clustered so that a better decision boundary can be learned to improve\\nit's generalization. Extensive experiments on classification and segmentation\\nhave shown that PDEN can achieve up to 15.28% improvement compared with the\\nstate-of-the-art single-domain generalization methods.\"\n",
      " b'Arbitrary-oriented objects exist widely in natural scenes, and thus the\\noriented object detection has received extensive attention in recent years. The\\nmainstream rotation detectors use oriented bounding boxes (OBB) or\\nquadrilateral bounding boxes (QBB) to represent the rotating objects. However,\\nthese methods suffer from the representation ambiguity for oriented object\\ndefinition, which leads to suboptimal regression optimization and the\\ninconsistency between the loss metric and the localization accuracy of the\\npredictions. In this paper, we propose a Representation Invariance Loss (RIL)\\nto optimize the bounding box regression for the rotating objects. Specifically,\\nRIL treats multiple representations of an oriented object as multiple\\nequivalent local minima, and hence transforms bounding box regression into an\\nadaptive matching process with these local minima. Then, the Hungarian matching\\nalgorithm is adopted to obtain the optimal regression strategy. We also propose\\na normalized rotation loss to alleviate the weak correlation between different\\nvariables and their unbalanced loss contribution in OBB representation.\\nExtensive experiments on remote sensing datasets and scene text datasets show\\nthat our method achieves consistent and substantial improvement. The source\\ncode and trained models are available at https://github.com/ming71/RIDet.'\n",
      " b'Although convolutional neural networks have achieved remarkable success in\\nanalyzing 2D images/videos, it is still non-trivial to apply the well-developed\\n2D techniques in regular domains to the irregular 3D point cloud data. To\\nbridge this gap, we propose ParaNet, a novel end-to-end deep learning\\nframework, for representing 3D point clouds in a completely regular and nearly\\nlossless manner. To be specific, ParaNet converts an irregular 3D point cloud\\ninto a regular 2D color image, named point geometry image (PGI), where each\\npixel encodes the spatial coordinates of a point. In contrast to conventional\\nregular representation modalities based on multi-view projection and\\nvoxelization, the proposed representation is differentiable and reversible.\\nTechnically, ParaNet is composed of a surface embedding module, which\\nparameterizes 3D surface points onto a unit square, and a grid resampling\\nmodule, which resamples the embedded 2D manifold over regular dense grids. Note\\nthat ParaNet is unsupervised, i.e., the training simply relies on\\nreference-free geometry constraints. The PGIs can be seamlessly coupled with a\\ntask network established upon standard and mature techniques for 2D\\nimages/videos to realize a specific task for 3D point clouds. We evaluate\\nParaNet over shape classification and point cloud upsampling, in which our\\nsolutions perform favorably against the existing state-of-the-art methods. We\\nbelieve such a paradigm will open up many possibilities to advance the progress\\nof deep learning-based point cloud processing and understanding.'\n",
      " b'Learning from visual observations is a fundamental yet challenging problem in\\nReinforcement Learning (RL). Although algorithmic advances combined with\\nconvolutional neural networks have proved to be a recipe for success, current\\nmethods are still lacking on two fronts: (a) data-efficiency of learning and\\n(b) generalization to new environments. To this end, we present Reinforcement\\nLearning with Augmented Data (RAD), a simple plug-and-play module that can\\nenhance most RL algorithms. We perform the first extensive study of general\\ndata augmentations for RL on both pixel-based and state-based inputs, and\\nintroduce two new data augmentations - random translate and random amplitude\\nscale. We show that augmentations such as random translate, crop, color jitter,\\npatch cutout, random convolutions, and amplitude scale can enable simple RL\\nalgorithms to outperform complex state-of-the-art methods across common\\nbenchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and\\nfinal performance on the DeepMind Control Suite benchmark for pixel-based\\ncontrol as well as OpenAI Gym benchmark for state-based control. We further\\ndemonstrate that RAD significantly improves test-time generalization over\\nexisting methods on several OpenAI ProcGen benchmarks. Our RAD module and\\ntraining code are available at https://www.github.com/MishaLaskin/rad.'\n",
      " b'In this paper, we consider the problem of event classification with\\nmulti-variate time series data consisting of heterogeneous (continuous and\\ncategorical) variables. The complex temporal dependencies between the variables\\ncombined with sparsity of the data makes the event classification problem\\nparticularly challenging. Most state-of-art approaches address this either by\\ndesigning hand-engineered features or breaking up the problem over homogeneous\\nvariates. In this work, we propose and compare three representation learning\\nalgorithms over symbolized sequences which enables classification of\\nheterogeneous time-series data using a deep architecture. The proposed\\nrepresentations are trained jointly along with the rest of the network\\narchitecture in an end-to-end fashion that makes the learned features\\ndiscriminative for the given task. Experiments on three real-world datasets\\ndemonstrate the effectiveness of the proposed approaches.'\n",
      " b'Learning text representation is crucial for text classification and other\\nlanguage related tasks. There are a diverse set of text representation networks\\nin the literature, and how to find the optimal one is a non-trivial problem.\\nRecently, the emerging Neural Architecture Search (NAS) techniques have\\ndemonstrated good potential to solve the problem. Nevertheless, most of the\\nexisting works of NAS focus on the search algorithms and pay little attention\\nto the search space. In this paper, we argue that the search space is also an\\nimportant human prior to the success of NAS in different applications. Thus, we\\npropose a novel search space tailored for text representation. Through\\nautomatic search, the discovered network architecture outperforms\\nstate-of-the-art models on various public datasets on text classification and\\nnatural language inference tasks. Furthermore, some of the design principles\\nfound in the automatic network agree well with human intuition.'\n",
      " b'Within the past decade, the rise of applications based on artificial\\nintelligence (AI) in general and machine learning (ML) in specific has led to\\nmany significant contributions within different domains. The applications range\\nfrom robotics over medical diagnoses up to autonomous driving. However, nearly\\nall applications rely on trained data. In case this data consists of 3D images,\\nit is of utmost importance that the labeling is as accurate as possible to\\nensure high-quality outcomes of the ML models. Labeling in the 3D space is\\nmostly manual work performed by expert workers, where they draw 3D bounding\\nboxes around target objects the ML model should later automatically identify,\\ne.g., pedestrians for autonomous driving or cancer cells within radiography.\\n  While a small range of recent 3D labeling tools exist, they all share three\\nmajor shortcomings: (i) they are specified for autonomous driving applications,\\n(ii) they lack convenience and comfort functions, and (iii) they have high\\ndependencies and little flexibility in data format. Therefore, we propose a\\nnovel labeling tool for 3D object detection in point clouds to address these\\nshortcomings.'\n",
      " b'One of the common obstacles for learning causal models from data is that\\nhigh-order conditional independence (CI) relationships between random variables\\nare difficult to estimate. Since CI tests with conditioning sets of low order\\ncan be performed accurately even for a small number of observations, a\\nreasonable approach to determine casual structures is to base merely on the\\nlow-order CIs. Recent research has confirmed that, e.g. in the case of sparse\\ntrue causal models, structures learned even from zero- and first-order\\nconditional independencies yield good approximations of the models. However, a\\nchallenging task here is to provide methods that faithfully explain a given set\\nof low-order CIs. In this paper, we propose an algorithm which, for a given set\\nof conditional independencies of order less or equal to $k$, where $k$ is a\\nsmall fixed number, computes a faithful graphical representation of the given\\nset. Our results complete and generalize the previous work on learning from\\npairwise marginal independencies. Moreover, they enable to improve upon the 0-1\\ngraph model which, e.g. is heavily used in the estimation of genome networks.'\n",
      " b'Visual navigation in complex environments is inefficient with traditional\\nreactive policy or general-purposed recurrent policy. To address the long-term\\nmemory issue, this paper proposes a graph attention memory (GAM) architecture\\nconsisting of memory construction module, graph attention module and control\\nmodule. The memory construction module builds the topological graph based on\\nsupervised learning by taking the exploration prior. Then, guided attention\\nfeatures are extracted with the graph attention module. Finally, the deep\\nreinforcement learning based control module makes decisions based on visual\\nobservations and guided attention features. Detailed convergence analysis of\\nGAM is presented in this paper. We evaluate GAM-based navigation system in two\\ncomplex 3D environments. Experimental results show that the GAM-based\\nnavigation system significantly improves learning efficiency and outperforms\\nall baselines in average success rate.'\n",
      " b\"We propose a novel interactive learning framework which we refer to as\\nInteractive Attention Learning (IAL), in which the human supervisors\\ninteractively manipulate the allocated attentions, to correct the model's\\nbehavior by updating the attention-generating network. However, such a model is\\nprone to overfitting due to scarcity of human annotations, and requires costly\\nretraining. Moreover, it is almost infeasible for the human annotators to\\nexamine attentions on tons of instances and features. We tackle these\\nchallenges by proposing a sample-efficient attention mechanism and a\\ncost-effective reranking algorithm for instances and features. First, we\\npropose Neural Attention Process (NAP), which is an attention generator that\\ncan update its behavior by incorporating new attention-level supervisions\\nwithout any retraining. Secondly, we propose an algorithm which prioritizes the\\ninstances and the features by their negative impacts, such that the model can\\nyield large improvements with minimal human feedback. We validate IAL on\\nvarious time-series datasets from multiple domains (healthcare, real-estate,\\nand computer vision) on which it significantly outperforms baselines with\\nconventional attention mechanisms, or without cost-effective reranking, with\\nsubstantially less retraining and human-model interaction cost.\"\n",
      " b'Learning structures of 3D shapes is a fundamental problem in the field of\\ncomputer graphics and geometry processing. We present a simple yet\\ninterpretable unsupervised method for learning a new structural representation\\nin the form of 3D structure points. The 3D structure points produced by our\\nmethod encode the shape structure intrinsically and exhibit semantic\\nconsistency across all the shape instances with similar structures. This is a\\nchallenging goal that has not fully been achieved by other methods.\\nSpecifically, our method takes a 3D point cloud as input and encodes it as a\\nset of local features. The local features are then passed through a novel point\\nintegration module to produce a set of 3D structure points. The chamfer\\ndistance is used as reconstruction loss to ensure the structure points lie\\nclose to the input point cloud. Extensive experiments have shown that our\\nmethod outperforms the state-of-the-art on the semantic shape correspondence\\ntask and achieves comparable performance with the state-of-the-art on the\\nsegmentation label transfer task. Moreover, the PCA based shape embedding built\\nupon consistent structure points demonstrates good performance in preserving\\nthe shape structures. Code is available at\\nhttps://github.com/NolenChen/3DStructurePoints'\n",
      " b'We tackle the challenge of Visual Question Answering in multi-image setting\\nfor the ISVQA dataset. Traditional VQA tasks have focused on a single-image\\nsetting where the target answer is generated from a single image. Image set\\nVQA, however, comprises of a set of images and requires finding connection\\nbetween images, relate the objects across images based on these connections and\\ngenerate a unified answer. In this report, we work with 4 approaches in a bid\\nto improve the performance on the task. We analyse and compare our results with\\nthree baseline models - LXMERT, HME-VideoQA and VisualBERT - and show that our\\napproaches can provide a slight improvement over the baselines. In specific, we\\ntry to improve on the spatial awareness of the model and help the model\\nidentify color using enhanced pre-training, reduce language dependence using\\nadversarial regularization, and improve counting using regression loss and\\ngraph based deduplication. We further delve into an in-depth analysis on the\\nlanguage bias in the ISVQA dataset and show how models trained on ISVQA\\nimplicitly learn to associate language more strongly with the final answer.'\n",
      " b'Automatic parsing of anatomical objects in X-ray images is critical to many\\nclinical applications in particular towards image-guided invention and workflow\\nautomation. Existing deep network models require a large amount of labeled\\ndata. However, obtaining accurate pixel-wise labeling in X-ray images relies\\nheavily on skilled clinicians due to the large overlaps of anatomy and the\\ncomplex texture patterns. On the other hand, organs in 3D CT scans preserve\\nclearer structures as well as sharper boundaries and thus can be easily\\ndelineated. In this paper, we propose a novel model framework for learning\\nautomatic X-ray image parsing from labeled CT scans. Specifically, a Dense\\nImage-to-Image network (DI2I) for multi-organ segmentation is first trained on\\nX-ray like Digitally Reconstructed Radiographs (DRRs) rendered from 3D CT\\nvolumes. Then we introduce a Task Driven Generative Adversarial Network\\n(TD-GAN) architecture to achieve simultaneous style transfer and parsing for\\nunseen real X-ray images. TD-GAN consists of a modified cycle-GAN substructure\\nfor pixel-to-pixel translation between DRRs and X-ray images and an added\\nmodule leveraging the pre-trained DI2I to enforce segmentation consistency. The\\nTD-GAN framework is general and can be easily adapted to other learning tasks.\\nIn the numerical experiments, we validate the proposed model on 815 DRRs and\\n153 topograms. While the vanilla DI2I without any adaptation fails completely\\non segmenting the topograms, the proposed model does not require any topogram\\nlabels and is able to provide a promising average dice of 85% which achieves\\nthe same level accuracy of supervised training (88%).'\n",
      " b'In this work, we tackle the essential problem of scale inconsistency for\\nself-supervised joint depth-pose learning. Most existing methods assume that a\\nconsistent scale of depth and pose can be learned across all input samples,\\nwhich makes the learning problem harder, resulting in degraded performance and\\nlimited generalization in indoor environments and long-sequence visual odometry\\napplication. To address this issue, we propose a novel system that explicitly\\ndisentangles scale from the network estimation. Instead of relying on PoseNet\\narchitecture, our method recovers relative pose by directly solving fundamental\\nmatrix from dense optical flow correspondence and makes use of a two-view\\ntriangulation module to recover an up-to-scale 3D structure. Then, we align the\\nscale of the depth prediction with the triangulated point cloud and use the\\ntransformed depth map for depth error computation and dense reprojection check.\\nOur whole system can be jointly trained end-to-end. Extensive experiments show\\nthat our system not only reaches state-of-the-art performance on KITTI depth\\nand flow estimation, but also significantly improves the generalization ability\\nof existing self-supervised depth-pose learning methods under a variety of\\nchallenging scenarios, and achieves state-of-the-art results among\\nself-supervised learning-based methods on KITTI Odometry and NYUv2 dataset.\\nFurthermore, we present some interesting findings on the limitation of\\nPoseNet-based relative pose estimation methods in terms of generalization\\nability. Code is available at https://github.com/B1ueber2y/TrianFlow.'\n",
      " b'We propose ST-DETR, a Spatio-Temporal Transformer-based architecture for\\nobject detection from a sequence of temporal frames. We treat the temporal\\nframes as sequences in both space and time and employ the full attention\\nmechanisms to take advantage of the features correlations over both dimensions.\\nThis treatment enables us to deal with frames sequence as temporal object\\nfeatures traces over every location in the space. We explore two possible\\napproaches; the early spatial features aggregation over the temporal dimension,\\nand the late temporal aggregation of object query spatial features. Moreover,\\nwe propose a novel Temporal Positional Embedding technique to encode the time\\nsequence information. To evaluate our approach, we choose the Moving Object\\nDetection (MOD)task, since it is a perfect candidate to showcase the importance\\nof the temporal dimension. Results show a significant 5% mAP improvement on the\\nKITTI MOD dataset over the 1-step spatial baseline.'\n",
      " b'Communication between agents in collaborative multi-agent settings is in\\ngeneral implicit or a direct data stream. This paper considers text-based\\nnatural language as a novel form of communication between multiple agents\\ntrained with reinforcement learning. This could be considered first steps\\ntoward a truly autonomous communication without the need to define a limited\\nset of instructions, and natural collaboration between humans and robots.\\nInspired by the game of Blind Leads, we propose an environment where one agent\\nuses natural language instructions to guide another through a maze. We test the\\nability of reinforcement learning agents to effectively communicate through\\ndiscrete word-level symbols and show that the agents are able to sufficiently\\ncommunicate through natural language with a limited vocabulary. Although the\\ncommunication is not always perfect English, the agents are still able to\\nnavigate the maze. We achieve a BLEU score of 0.85, which is an improvement of\\n0.61 over randomly generated sequences while maintaining a 100% maze completion\\nrate. This is a 3.5 times the performance of the random baseline using our\\nreference set.'\n",
      " b'Convolutional Neural Networks (CNN) are successfully used for various visual\\nperception tasks including bounding box object detection, semantic\\nsegmentation, optical flow, depth estimation and visual SLAM. Generally these\\ntasks are independently explored and modeled. In this paper, we present a joint\\nmulti-task network design for learning object detection and semantic\\nsegmentation simultaneously. The main motivation is to achieve real-time\\nperformance on a low power embedded SOC by sharing of encoder for both the\\ntasks. We construct an efficient architecture using a small ResNet10 like\\nencoder which is shared for both decoders. Object detection uses YOLO v2 like\\ndecoder and semantic segmentation uses FCN8 like decoder. We evaluate the\\nproposed network in two public datasets (KITTI, Cityscapes) and in our private\\nfisheye camera dataset, and demonstrate that joint network provides the same\\naccuracy as that of separate networks. We further optimize the network to\\nachieve 30 fps for 1280x384 resolution image.'\n",
      " b'Recently there has been an enormous interest in generative models for images\\nin deep learning. In pursuit of this, Generative Adversarial Networks (GAN) and\\nVariational Auto-Encoder (VAE) have surfaced as two most prominent and popular\\nmodels. While VAEs tend to produce excellent reconstructions but blurry\\nsamples, GANs generate sharp but slightly distorted images. In this paper we\\npropose a new model called Variational InfoGAN (ViGAN). Our aim is two fold:\\n(i) To generated new images conditioned on visual descriptions, and (ii) modify\\nthe image, by fixing the latent representation of image and varying the visual\\ndescription. We evaluate our model on Labeled Faces in the Wild (LFW), celebA\\nand a modified version of MNIST datasets and demonstrate the ability of our\\nmodel to generate new images as well as to modify a given image by changing\\nattributes.'\n",
      " b\"Time Series Classification (TSC) has been an important and challenging task\\nin data mining, especially on multivariate time series and multi-view time\\nseries data sets. Meanwhile, transfer learning has been widely applied in\\ncomputer vision and natural language processing applications to improve deep\\nneural network's generalization capabilities. However, very few previous works\\napplied transfer learning framework to time series mining problems.\\nParticularly, the technique of measuring similarities between source domain and\\ntarget domain based on dynamic representation such as density estimation with\\nimportance sampling has never been combined with transfer learning framework.\\nIn this paper, we first proposed a general adaptive transfer learning framework\\nfor multi-view time series data, which shows strong ability in storing\\ninter-view importance value in the process of knowledge transfer. Next, we\\nrepresented inter-view importance through some time series similarity\\nmeasurements and approximated the posterior distribution in latent space for\\nthe importance sampling via density estimation techniques. We then computed the\\nmatrix norm of sampled importance value, which controls the degree of knowledge\\ntransfer in pre-training process. We further evaluated our work, applied it to\\nmany other time series classification tasks, and observed that our architecture\\nmaintained desirable generalization ability. Finally, we concluded that our\\nframework could be adapted with deep learning techniques to receive significant\\nmodel performance improvements.\"\n",
      " b'Learning Enabled Components (LECs) are widely being used in a variety of\\nperception based autonomy tasks like image segmentation, object detection,\\nend-to-end driving, etc. These components are trained with large image datasets\\nwith multimodal factors like weather conditions, time-of-day, traffic-density,\\netc. The LECs learn from these factors during training, and while testing if\\nthere is variation in any of these factors, the components get confused\\nresulting in low confidence predictions. The images with factors not seen\\nduring training is commonly referred to as Out-of-Distribution (OOD). For safe\\nautonomy it is important to identify the OOD images, so that a suitable\\nmitigation strategy can be performed. Classical one-class classifiers like SVM\\nand SVDD are used to perform OOD detection. However, the multiple labels\\nattached to the images in these datasets, restricts the direct application of\\nthese techniques. We address this problem using the latent space of the\\n$\\\\beta$-Variational Autoencoder ($\\\\beta$-VAE). We use the fact that compact\\nlatent space generated by an appropriately selected $\\\\beta$-VAE will encode the\\ninformation about these factors in a few latent variables, and that can be used\\nfor computationally inexpensive detection. We evaluate our approach on the\\nnuScenes dataset, and our results shows the latent space of $\\\\beta$-VAE is\\nsensitive to encode changes in the values of the generative factor.'\n",
      " b'We introduce three new generative models for time series. Based on Euler\\ndiscretization and Wasserstein metrics, they are able to capture time marginal\\ndistributions and temporal dynamics. Two of these methods rely on the\\nadaptation of generative adversarial networks (GANs) to time series. Both of\\nthem outperform state-of-the-art benchmarks by capturing the underlying\\ntemporal structure on synthetic time series. The third algorithm, called\\nConditional Euler Generator (CEGEN), minimizes a dedicated distance between the\\ntransition probability distributions over all time steps. In the context of Ito\\nprocesses, we provide theoretical guarantees that minimizing this criterion\\nimplies accurate estimations of the drift and volatility parameters. We\\ndemonstrate empirically that CEGEN outperforms state-of-the-art and GAN\\ngenerators on both marginal and temporal dynamics metrics. Besides, it\\nidentifies accurate correlation structures in high dimension. When few data\\npoints are available, we verify the effectiveness of CEGEN, when combined with\\ntransfer learning methods on Monte Carlo simulations. Finally, we illustrate\\nthe robustness of our method on various real-world datasets.'\n",
      " b'In this work, we study the image transformation problem by learning the\\nunderlying transformations from a collection of images using Generative\\nAdversarial Networks (GANs). Specifically, we propose an unsupervised learning\\nframework, termed as TrGAN, to project images onto a transformation space that\\nis shared by the generator and the discriminator. Any two points in this\\nprojected space define a transformation that can guide the image generation\\nprocess, leading to continuous semantic change. By projecting a pair of images\\nonto the transformation space, we are able to adequately extract the semantic\\nvariation between them and further apply the extracted semantic to facilitating\\nimage editing, including not only transferring image styles (e.g., changing day\\nto night) but also manipulating image contents (e.g., adding clouds in the\\nsky). Code and models are available at https://genforce.github.io/trgan.'\n",
      " b\"The task of multi-label image classification is to recognize all the object\\nlabels presented in an image. Though advancing for years, small objects,\\nsimilar objects and objects with high conditional probability are still the\\nmain bottlenecks of previous convolutional neural network(CNN) based models,\\nlimited by convolutional kernels' representational capacity. Recent vision\\ntransformer networks utilize the self-attention mechanism to extract the\\nfeature of pixel granularity, which expresses richer local semantic\\ninformation, while is insufficient for mining global spatial dependence. In\\nthis paper, we point out the three crucial problems that CNN-based methods\\nencounter and explore the possibility of conducting specific transformer\\nmodules to settle them. We put forward a Multi-label Transformer\\narchitecture(MlTr) constructed with windows partitioning, in-window pixel\\nattention, cross-window attention, particularly improving the performance of\\nmulti-label image classification tasks. The proposed MlTr shows\\nstate-of-the-art results on various prevalent multi-label datasets such as\\nMS-COCO, Pascal-VOC, and NUS-WIDE with 88.5%, 95.8%, and 65.5% respectively.\\nThe code will be available soon at https://github.com/starmemda/MlTr/\"\n",
      " b'Traffic signal control is one of the most effective methods of traffic\\nmanagement in urban areas. In recent years, traffic control methods based on\\ndeep reinforcement learning (DRL) have gained attention due to their ability to\\nexploit real-time traffic data, which is often poorly used by the traditional\\nhand-crafted methods. While most recent DRL-based methods have focused on\\nmaximizing the throughput or minimizing the average travel time of the\\nvehicles, the fairness of the traffic signal controllers has often been\\nneglected. This is particularly important as neglecting fairness can lead to\\nsituations where some vehicles experience extreme waiting times, or where the\\nthroughput of a particular traffic flow is highly impacted by the fluctuations\\nof another conflicting flow at the intersection. In order to address these\\nissues, we introduce two notions of fairness: delay-based and throughput-based\\nfairness, which correspond to the two issues mentioned above. Furthermore, we\\npropose two DRL-based traffic signal control methods for implementing these\\nfairness notions, that can achieve a high throughput as well. We evaluate the\\nperformance of our proposed methods using three traffic arrival distributions,\\nand find that our methods outperform the baselines in the tested scenarios.'\n",
      " b'Graph neural networks for heterogeneous graph embedding is to project nodes\\ninto a low-dimensional space by exploring the heterogeneity and semantics of\\nthe heterogeneous graph. However, on the one hand, most of existing\\nheterogeneous graph embedding methods either insufficiently model the local\\nstructure under specific semantic, or neglect the heterogeneity when\\naggregating information from it. On the other hand, representations from\\nmultiple semantics are not comprehensively integrated to obtain versatile node\\nembeddings. To address the problem, we propose a Heterogeneous Graph Neural\\nNetwork with Multi-View Representation Learning (named MV-HetGNN) for\\nheterogeneous graph embedding by introducing the idea of multi-view\\nrepresentation learning. The proposed model consists of node feature\\ntransformation, view-specific ego graph encoding and auto multi-view fusion to\\nthoroughly learn complex structural and semantic information for generating\\ncomprehensive node representations. Extensive experiments on three real-world\\nheterogeneous graph datasets show that the proposed MV-HetGNN model\\nconsistently outperforms all the state-of-the-art GNN baselines in various\\ndownstream tasks, e.g., node classification, node clustering, and link\\nprediction.'\n",
      " b\"Deep neural network approaches have demonstrated high performance in object\\nrecognition (CNN) and detection (Faster-RCNN) tasks, but experiments have shown\\nthat such architectures are vulnerable to adversarial attacks (FFF, UAP): low\\namplitude perturbations, barely perceptible by the human eye, can lead to a\\ndrastic reduction in labeling performance. This article proposes a new context\\nmodule, called \\\\textit{Transformer-Encoder Detector Module}, that can be\\napplied to an object detector to (i) improve the labeling of object instances;\\nand (ii) improve the detector's robustness to adversarial attacks. The proposed\\nmodel achieves higher mAP, F1 scores and AUC average score of up to 13\\\\%\\ncompared to the baseline Faster-RCNN detector, and an mAP score 8 points higher\\non images subjected to FFF or UAP attacks due to the inclusion of both\\ncontextual and visual features extracted from scene and encoded into the model.\\nThe result demonstrates that a simple ad-hoc context module can improve the\\nreliability of object detectors significantly.\"\n",
      " b'A major endeavor of computer vision is to represent, understand and extract\\nstructure from 3D data. Towards this goal, unsupervised learning is a powerful\\nand necessary tool. Most current unsupervised methods for 3D shape analysis use\\ndatasets that are aligned, require objects to be reconstructed and suffer from\\ndeteriorated performance on downstream tasks. To solve these issues, we propose\\nto extend the InfoMax and contrastive learning principles on 3D shapes. We show\\nthat we can maximize the mutual information between 3D objects and their\\n\"chunks\" to improve the representations in aligned datasets. Furthermore, we\\ncan achieve rotation invariance in SO${(3)}$ group by maximizing the mutual\\ninformation between the 3D objects and their geometric transformed versions.\\nFinally, we conduct several experiments such as clustering, transfer learning,\\nshape retrieval, and achieve state of art results.'\n",
      " b'Adverse Drug Reaction (ADR) is a significant public health concern\\nworld-wide. Numerous graph-based methods have been applied to biomedical graphs\\nfor predicting ADRs in pre-marketing phases. ADR detection in post-market\\nsurveillance is no less important than pre-marketing assessment, and ADR\\ndetection with large-scale clinical data have attracted much attention in\\nrecent years. However, there are not many studies considering graph structures\\nfrom clinical data for detecting an ADR signal, which is a pair of a\\nprescription and a diagnosis that might be a potential ADR. In this study, we\\ndevelop a novel graph-based framework for ADR signal detection using healthcare\\nclaims data. We construct a Drug-disease graph with nodes representing the\\nmedical codes. The edges are given as the relationships between two codes,\\ncomputed using the data. We apply Graph Neural Network to predict ADR signals,\\nusing labels from the Side Effect Resource database. The model shows improved\\nAUROC and AUPRC performance of 0.795 and 0.775, compared to other algorithms,\\nshowing that it successfully learns node representations expressive of those\\nrelationships. Furthermore, our model predicts ADR pairs that do not exist in\\nthe established ADR database, showing its capability to supplement the ADR\\ndatabase.'\n",
      " b'Existing person re-identification (re-id) methods assume the provision of\\naccurately cropped person bounding boxes with minimum background noise, mostly\\nby manually cropping. This is significantly breached in practice when person\\nbounding boxes must be detected automatically given a very large number of\\nimages and/or videos processed. Compared to carefully cropped manually,\\nauto-detected bounding boxes are far less accurate with random amount of\\nbackground clutter which can degrade notably person re-id matching accuracy. In\\nthis work, we develop a joint learning deep model that optimises person re-id\\nattention selection within any auto-detected person bounding boxes by\\nreinforcement learning of background clutter minimisation subject to re-id\\nlabel pairwise constraints. Specifically, we formulate a novel unified re-id\\narchitecture called Identity DiscriminativE Attention reinforcement Learning\\n(IDEAL) to accurately select re-id attention in auto-detected bounding boxes\\nfor optimising re-id performance. Our model can improve re-id accuracy\\ncomparable to that from exhaustive human manual cropping of bounding boxes with\\nadditional advantages from identity discriminative attention selection that\\nspecially benefits re-id tasks beyond human knowledge. Extensive comparative\\nevaluations demonstrate the re-id advantages of the proposed IDEAL model over a\\nwide range of state-of-the-art re-id methods on two auto-detected re-id\\nbenchmarks CUHK03 and Market-1501.'\n",
      " b'Many tasks in computer vision are often calibrated and evaluated relative to\\nhuman perception. In this paper, we propose to directly approximate the\\nperceptual function performed by human observers completing a visual detection\\ntask. Specifically, we present a novel methodology for learning to detect image\\ntransformations visible to human observers through approximating perceptual\\nthresholds. To do this, we carry out a subjective two-alternative forced-choice\\nstudy to estimate perceptual thresholds of human observers detecting local\\nexposure shifts in images. We then leverage transformation equivariant\\nrepresentation learning to overcome issues of limited perceptual data. This\\nrepresentation is then used to train a dense convolutional classifier capable\\nof detecting local suprathreshold exposure shifts - a distortion common to\\nimage composites. In this context, our model can approximate perceptual\\nthresholds with an average error of 0.1148 exposure stops between empirical and\\npredicted thresholds. It can also be trained to detect a range of different\\nlocal transformations.'\n",
      " b'Graph Attention Network (GAT) and GraphSAGE are neural network architectures\\nthat operate on graph-structured data and have been widely studied for link\\nprediction and node classification. One challenge raised by GraphSAGE is how to\\nsmartly combine neighbour features based on graph structure. GAT handles this\\nproblem through attention, however the challenge with GAT is its scalability\\nover large and dense graphs. In this work, we proposed a new architecture to\\naddress these issues that is more efficient and is capable of incorporating\\ndifferent edge type information. It generates node representations by attending\\nto neighbours sampled from weighted multi-step transition probabilities. We\\nconduct experiments on both transductive and inductive settings. Experiments\\nachieved comparable or better results on several graph benchmarks, including\\nthe Cora, Citeseer, Pubmed, PPI, Twitter, and YouTube datasets.'\n",
      " b'Credit card has become popular mode of payment for both online and offline\\npurchase, which leads to increasing daily fraud transactions. An Efficient\\nfraud detection methodology is therefore essential to maintain the reliability\\nof the payment system. In this study, we perform a comparison study of credit\\ncard fraud detection by using various supervised and unsupervised approaches.\\nSpecifically, 6 supervised classification models, i.e., Logistic Regression\\n(LR), K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Tree\\n(DT), Random Forest (RF), Extreme Gradient Boosting (XGB), as well as 4\\nunsupervised anomaly detection models, i.e., One-Class SVM (OCSVM),\\nAuto-Encoder (AE), Restricted Boltzmann Machine (RBM), and Generative\\nAdversarial Networks (GAN), are explored in this study. We train all these\\nmodels on a public credit card transaction dataset from Kaggle website, which\\ncontains 492 frauds out of 284,807 transactions. The labels of the transactions\\nare used for supervised learning models only. The performance of each model is\\nevaluated through 5-fold cross validation in terms of Area Under the Receiver\\nOperating Curves (AUROC). Within supervised approaches, XGB and RF obtain the\\nbest performance with AUROC = 0.989 and AUROC = 0.988, respectively. While for\\nunsupervised approaches, RBM achieves the best performance with AUROC = 0.961,\\nfollowed by GAN with AUROC = 0.954. The experimental results show that\\nsupervised models perform slightly better than unsupervised models in this\\nstudy. Anyway, unsupervised approaches are still promising for credit card\\nfraud transaction detection due to the insufficient annotation and the data\\nimbalance issue in real-world applications.'\n",
      " b'Generative Adversarial Networks (GANs) is a novel class of deep generative\\nmodels which has recently gained significant attention. GANs learns complex and\\nhigh-dimensional distributions implicitly over images, audio, and data.\\nHowever, there exists major challenges in training of GANs, i.e., mode\\ncollapse, non-convergence and instability, due to inappropriate design of\\nnetwork architecture, use of objective function and selection of optimization\\nalgorithm. Recently, to address these challenges, several solutions for better\\ndesign and optimization of GANs have been investigated based on techniques of\\nre-engineered network architectures, new objective functions and alternative\\noptimization algorithms. To the best of our knowledge, there is no existing\\nsurvey that has particularly focused on broad and systematic developments of\\nthese solutions. In this study, we perform a comprehensive survey of the\\nadvancements in GANs design and optimization solutions proposed to handle GANs\\nchallenges. We first identify key research issues within each design and\\noptimization technique and then propose a new taxonomy to structure solutions\\nby key research issues. In accordance with the taxonomy, we provide a detailed\\ndiscussion on different GANs variants proposed within each solution and their\\nrelationships. Finally, based on the insights gained, we present the promising\\nresearch directions in this rapidly growing field.'\n",
      " b'Diabetic retinopathy (DR) is a diabetes complication that affects eyes. DR is\\na primary cause of blindness in working-age people and it is estimated that 3\\nto 4 million people with diabetes are blinded by DR every year worldwide. Early\\ndiagnosis have been considered an effective way to mitigate such problem. The\\nultimate goal of our research is to develop novel machine learning techniques\\nto analyze the DR images generated by the fundus camera for automatically DR\\ndiagnosis. In this paper, we focus on identifying small lesions on DR fundus\\nimages. The results from our analysis, which include the lesion category and\\ntheir exact locations in the image, can be used to facilitate the determination\\nof DR severity (indicated by DR stages). Different from traditional object\\ndetection for natural images, lesion detection for fundus images have unique\\nchallenges. Specifically, the size of a lesion instance is usually very small,\\ncompared with the original resolution of the fundus images, making them\\ndiffcult to be detected. We analyze the lesion-vs-image scale carefully and\\npropose a large-size feature pyramid network (LFPN) to preserve more image\\ndetails for mini lesion instance detection. Our method includes an effective\\nregion proposal strategy to increase the sensitivity. The experimental results\\nshow that our proposed method is superior to the original feature pyramid\\nnetwork (FPN) method and Faster RCNN.'\n",
      " b'Motivated by the prevailing paradigm of using unsupervised learning for\\nefficient exploration in reinforcement learning (RL) problems\\n[tang2017exploration,bellemare2016unifying], we investigate when this paradigm\\nis provably efficient. We study episodic Markov decision processes with rich\\nobservations generated from a small number of latent states. We present a\\ngeneral algorithmic framework that is built upon two components: an\\nunsupervised learning algorithm and a no-regret tabular RL algorithm.\\nTheoretically, we prove that as long as the unsupervised learning algorithm\\nenjoys a polynomial sample complexity guarantee, we can find a near-optimal\\npolicy with sample complexity polynomial in the number of latent states, which\\nis significantly smaller than the number of observations. Empirically, we\\ninstantiate our framework on a class of hard exploration problems to\\ndemonstrate the practicality of our theory.'\n",
      " b'Recently there was an increasing interest in applications of graph neural\\nnetworks in non-Euclidean geometry; however, are non-Euclidean representations\\nalways useful for graph learning tasks? For different problems such as node\\nclassification and link prediction we compute hyperbolic embeddings and\\nconclude that for tasks that require global prediction consistency it might be\\nuseful to use non-Euclidean embeddings, while for other tasks Euclidean models\\nare superior. To do so we first fix an issue of the existing models associated\\nwith the optimization process at zero curvature. Current hyperbolic models deal\\nwith gradients at the origin in ad-hoc manner, which is inefficient and can\\nlead to numerical instabilities. We solve the instabilities of\\nkappa-Stereographic model at zero curvature cases and evaluate the approach of\\nembedding graphs into the manifold in several graph representation learning\\ntasks.'\n",
      " b'Facial expressions recognition (FER) of 3D face scans has received a\\nsignificant amount of attention in recent years. Most of the facial expression\\nrecognition methods have been proposed using mainly 2D images. These methods\\nsuffer from several issues like illumination changes and pose variations.\\nMoreover, 2D mapping from 3D images may lack some geometric and topological\\ncharacteristics of the face. Hence, to overcome this problem, a multi-modal 2D\\n+ 3D feature-based method is proposed. We extract shallow features from the 3D\\nimages, and deep features using Convolutional Neural Networks (CNN) from the\\ntransformed 2D images. Combining these features into a compact representation\\nuses covariance matrices as descriptors for both features instead of\\nsingle-handedly descriptors. A covariance matrix learning is used as a manifold\\nlayer to reduce the deep covariance matrices size and enhance their\\ndiscrimination power while preserving their manifold structure. We then use the\\nBag-of-Features (BoF) paradigm to quantize the covariance matrices after\\nflattening. Accordingly, we obtained two codebooks using shallow and deep\\nfeatures. The global codebook is then used to feed an SVM classifier. High\\nclassification performances have been achieved on the BU-3DFE and Bosphorus\\ndatasets compared to the state-of-the-art methods.'\n",
      " b\"Detection faults in seismic data is a crucial step for seismic structural\\ninterpretation, reservoir characterization and well placement. Some recent\\nworks regard it as an image segmentation task. The task of image segmentation\\nrequires huge labels, especially 3D seismic data, which has a complex structure\\nand lots of noise. Therefore, its annotation requires expert experience and a\\nhuge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to\\neffectively train 3D-CNN by some slices from 3D seismic data, so that the model\\ncan learn the segmentation of 3D seismic data from a few 2D slices. In order to\\nfully extract information from limited data and suppress seismic noise, we\\npropose an attention module that can be used for active supervision training\\nand embedded in the network. The attention heatmap label is generated by the\\noriginal label, and letting it supervise the attention module using the\\nlambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss\\nfunction, the method can extract 3D seismic features from a few 2D slice\\nlabels. And it also shows the advanced performance of the attention module,\\nwhich can significantly suppress the noise in the seismic data while increasing\\nthe model's sensitivity to the foreground. Finally, on the public test set, we\\nonly use the 2D slice labels training that accounts for 3.3% of the 3D volume\\nlabel, and achieve similar performance to the 3D volume label training.\"\n",
      " b'3D detection plays an indispensable role in environment perception. Due to\\nthe high cost of commonly used LiDAR sensor, stereo vision based 3D detection,\\nas an economical yet effective setting, attracts more attention recently. For\\nthese approaches based on 2D images, accurate depth information is the key to\\nachieve 3D detection, and most existing methods resort to a preliminary stage\\nfor depth estimation. They mainly focus on the global depth and neglect the\\nproperty of depth information in this specific task, namely, sparsity and\\nlocality, where exactly accurate depth is only needed for these 3D bounding\\nboxes. Motivated by this finding, we propose a stereo-image based anchor-free\\n3D detection method, called structure-aware stereo 3D detector (termed as\\nSIDE), where we explore the instance-level depth information via constructing\\nthe cost volume from RoIs of each object. Due to the information sparsity of\\nlocal cost volume, we further introduce match reweighting and structure-aware\\nattention, to make the depth information more concentrated. Experiments\\nconducted on the KITTI dataset show that our method achieves the\\nstate-of-the-art performance compared to existing methods without depth map\\nsupervision.'\n",
      " b'One of the important evidence in a crime scene that is normally overlooked\\nbut very important evidence is shoe print as the criminal is normally unaware\\nof the mask for this. In this paper we use image processing technique to\\nprocess reference shoe images to make it index-able for a search from the\\ndatabase the shoe print impressions available in the commercial market. This is\\nachieved first by converting the commercially available image through the\\nprocess of converting them to gray scale then apply image enhancement and\\nrestoration techniques and finally do image segmentation to store the segmented\\nparameter as index in the database storage. We use histogram method for image\\nenhancement, inverse filtering for image restoration and threshold method for\\nindexing. We use global threshold as index of the shoe print. The paper\\ndescribes this method and simulation results are included to validate the\\nmethod.'\n",
      " b'Product embeddings have been heavily investigated in the past few years,\\nserving as the cornerstone for a broad range of machine learning applications\\nin e-commerce. Despite the empirical success of product embeddings, little is\\nknown on how and why they work from the theoretical standpoint. Analogous\\nresults from the natural language processing (NLP) often rely on\\ndomain-specific properties that are not transferable to the e-commerce setting,\\nand the downstream tasks often focus on different aspects of the embeddings. We\\ntake an e-commerce-oriented view of the product embeddings and reveal a\\ncomplete theoretical view from both the representation learning and the\\nlearning theory perspective. We prove that product embeddings trained by the\\nwidely-adopted skip-gram negative sampling algorithm and its variants are\\nsufficient dimension reduction regarding a critical product relatedness\\nmeasure. The generalization performance in the downstream machine learning task\\nis controlled by the alignment between the embeddings and the product\\nrelatedness measure. Following the theoretical discoveries, we conduct\\nexploratory experiments that supports our theoretical insights for the product\\nembeddings.'\n",
      " b\"We present a method for localizing a single camera with respect to a point\\ncloud map in indoor and outdoor scenes. The problem is challenging because\\ncorrespondences of local invariant features are inconsistent across the domains\\nbetween image and 3D. The problem is even more challenging as the method must\\nhandle various environmental conditions such as illumination, weather, and\\nseasonal changes. Our method can match equirectangular images to the 3D range\\nprojections by extracting cross-domain symmetric place descriptors. Our key\\ninsight is to retain condition-invariant 3D geometry features from limited data\\nsamples while eliminating the condition-related features by a designed\\nGenerative Adversarial Network. Based on such features, we further design a\\nspherical convolution network to learn viewpoint-invariant symmetric place\\ndescriptors. We evaluate our method on extensive self-collected datasets, which\\ninvolve \\\\textit{Long-term} (variant appearance conditions),\\n\\\\textit{Large-scale} (up to $2km$ structure/unstructured environment), and\\n\\\\textit{Multistory} (four-floor confined space). Our method surpasses other\\ncurrent state-of-the-arts by achieving around $3$ times higher place retrievals\\nto inconsistent environments, and above $3$ times accuracy on online\\nlocalization. To highlight our method's generalization capabilities, we also\\nevaluate the recognition across different datasets. With a single trained\\nmodel, i3dLoc can demonstrate reliable visual localization in random\\nconditions.\"\n",
      " b'Representation learning over graph structured data has been mostly studied in\\nstatic graph settings while efforts for modeling dynamic graphs are still\\nscant. In this paper, we develop a novel hierarchical variational model that\\nintroduces additional latent random variables to jointly model the hidden\\nstates of a graph recurrent neural network (GRNN) to capture both topology and\\nnode attribute changes in dynamic graphs. We argue that the use of high-level\\nlatent random variables in this variational GRNN (VGRNN) can better capture\\npotential variability observed in dynamic graphs as well as the uncertainty of\\nnode latent representation. With semi-implicit variational inference developed\\nfor this new VGRNN architecture (SI-VGRNN), we show that flexible non-Gaussian\\nlatent representations can further help dynamic graph analytic tasks. Our\\nexperiments with multiple real-world dynamic graph datasets demonstrate that\\nSI-VGRNN and VGRNN consistently outperform the existing baseline and\\nstate-of-the-art methods by a significant margin in dynamic link prediction.'\n",
      " b'Graph representation learning nowadays becomes fundamental in analyzing\\ngraph-structured data. Inspired by recent success of contrastive methods, in\\nthis paper, we propose a novel framework for unsupervised graph representation\\nlearning by leveraging a contrastive objective at the node level. Specifically,\\nwe generate two graph views by corruption and learn node representations by\\nmaximizing the agreement of node representations in these two views. To provide\\ndiverse node contexts for the contrastive objective, we propose a hybrid scheme\\nfor generating graph views on both structure and attribute levels. Besides, we\\nprovide theoretical justification behind our motivation from two perspectives,\\nmutual information and the classical triplet loss. We perform empirical\\nexperiments on both transductive and inductive learning tasks using a variety\\nof real-world datasets. Experimental experiments demonstrate that despite its\\nsimplicity, our proposed method consistently outperforms existing\\nstate-of-the-art methods by large margins. Moreover, our unsupervised method\\neven surpasses its supervised counterparts on transductive tasks, demonstrating\\nits great potential in real-world applications.'\n",
      " b'In the field of computer vision, unsupervised learning for 2D object\\ngeneration has advanced rapidly in the past few years. However, 3D object\\ngeneration has not garnered the same attention or success as its predecessor.\\nTo facilitate novel progress at the intersection of computer vision and\\nmaterials science, we propose a 3DMaterialGAN network that is capable of\\nrecognizing and synthesizing individual grains whose morphology conforms to a\\ngiven 3D polycrystalline material microstructure. This Generative Adversarial\\nNetwork (GAN) architecture yields complex 3D objects from probabilistic latent\\nspace vectors with no additional information from 2D rendered images. We show\\nthat this method performs comparably or better than state-of-the-art on\\nbenchmark annotated 3D datasets, while also being able to distinguish and\\ngenerate objects that are not easily annotated, such as grain morphologies. The\\nvalue of our algorithm is demonstrated with analysis on experimental real-world\\ndata, namely generating 3D grain structures found in a commercially relevant\\nwrought titanium alloy, which were validated through statistical shape\\ncomparison. This framework lays the foundation for the recognition and\\nsynthesis of polycrystalline material microstructures, which are used in\\nadditive manufacturing, aerospace, and structural design applications.'\n",
      " b'With the resurgence of interest in neural networks, representation learning\\nhas re-emerged as a central focus in artificial intelligence. Representation\\nlearning refers to the discovery of useful encodings of data that make\\ndomain-relevant information explicit. Factorial representations identify\\nunderlying independent causal factors of variation in data. A factorial\\nrepresentation is compact and faithful, makes the causal factors explicit, and\\nfacilitates human interpretation of data. Factorial representations support a\\nvariety of applications, including the generation of novel examples, indexing\\nand search, novelty detection, and transfer learning.\\n  This article surveys various constraints that encourage a learning algorithm\\nto discover factorial representations. I dichotomize the constraints in terms\\nof unsupervised and supervised inductive bias. Unsupervised inductive biases\\nexploit assumptions about the environment, such as the statistical distribution\\nof factor coefficients, assumptions about the perturbations a factor should be\\ninvariant to (e.g. a representation of an object can be invariant to rotation,\\ntranslation or scaling), and assumptions about how factors are combined to\\nsynthesize an observation. Supervised inductive biases are constraints on the\\nrepresentations based on additional information connected to observations.\\nSupervisory labels come in variety of types, which vary in how strongly they\\nconstrain the representation, how many factors are labeled, how many\\nobservations are labeled, and whether or not we know the associations between\\nthe constraints and the factors they are related to.\\n  This survey brings together a wide variety of models that all touch on the\\nproblem of learning factorial representations and lays out a framework for\\ncomparing these models based on the strengths of the underlying supervised and\\nunsupervised inductive biases.'\n",
      " b'The adversarial vulnerability of deep neural networks has attracted\\nsignificant attention in machine learning. From a causal viewpoint, adversarial\\nattacks can be considered as a specific type of distribution change on natural\\ndata. As causal reasoning has an instinct for modeling distribution change, we\\npropose to incorporate causality into mitigating adversarial vulnerability.\\nHowever, causal formulations of the intuition of adversarial attack and the\\ndevelopment of robust DNNs are still lacking in the literature. To bridge this\\ngap, we construct a causal graph to model the generation process of adversarial\\nexamples and define the adversarial distribution to formalize the intuition of\\nadversarial attacks. From a causal perspective, we find that the label is\\nspuriously correlated with the style (content-independent) information when an\\ninstance is given. The spurious correlation implies that the adversarial\\ndistribution is constructed via making the statistical conditional association\\nbetween style information and labels drastically different from that in natural\\ndistribution. Thus, DNNs that fit the spurious correlation are vulnerable to\\nthe adversarial distribution. Inspired by the observation, we propose the\\nadversarial distribution alignment method to eliminate the difference between\\nthe natural distribution and the adversarial distribution. Extensive\\nexperiments demonstrate the efficacy of the proposed method. Our method can be\\nseen as the first attempt to leverage causality for mitigating adversarial\\nvulnerability.'\n",
      " b'Image processing is an important research area in computer vision. Image\\nsegmentation plays the vital rule in image processing research. There exist so\\nmany methods for image segmentation. Clustering is an unsupervised study.\\nClustering can also be used for image segmentation. In this paper, an in-depth\\nstudy is done on different clustering techniques that can be used for image\\nsegmentation with their pros and cons. An experiment for color image\\nsegmentation based on clustering with K-Means algorithm is performed to observe\\nthe accuracy of clustering technique for the segmentation purpose.'\n",
      " b'Despite the wealth of research into provably efficient reinforcement learning\\nalgorithms, most works focus on tabular representation and thus struggle to\\nhandle exponentially or infinitely large state-action spaces. In this paper, we\\nconsider episodic reinforcement learning with a continuous state-action space\\nwhich is assumed to be equipped with a natural metric that characterizes the\\nproximity between different states and actions. We propose ZoomRL, an online\\nalgorithm that leverages ideas from continuous bandits to learn an adaptive\\ndiscretization of the joint space by zooming in more promising and frequently\\nvisited regions while carefully balancing the exploitation-exploration\\ntrade-off. We show that ZoomRL achieves a worst-case regret\\n$\\\\tilde{O}(H^{\\\\frac{5}{2}} K^{\\\\frac{d+1}{d+2}})$ where $H$ is the planning\\nhorizon, $K$ is the number of episodes and $d$ is the covering dimension of the\\nspace with respect to the metric. Moreover, our algorithm enjoys improved\\nmetric-dependent guarantees that reflect the geometry of the underlying space.\\nFinally, we show that our algorithm is robust to small misspecification errors.'\n",
      " b'In this paper, we focus on three problems in deep learning based medical\\nimage segmentation. Firstly, U-net, as a popular model for medical image\\nsegmentation, is difficult to train when convolutional layers increase even\\nthough a deeper network usually has a better generalization ability because of\\nmore learnable parameters. Secondly, the exponential ReLU (ELU), as an\\nalternative of ReLU, is not much different from ReLU when the network of\\ninterest gets deep. Thirdly, the Dice loss, as one of the pervasive loss\\nfunctions for medical image segmentation, is not effective when the prediction\\nis close to ground truth and will cause oscillation during training. To address\\nthe aforementioned three problems, we propose and validate a deeper network\\nthat can fit medical image datasets that are usually small in the sample size.\\nMeanwhile, we propose a new loss function to accelerate the learning process\\nand a combination of different activation functions to improve the network\\nperformance. Our experimental results suggest that our network is comparable or\\nsuperior to state-of-the-art methods.'\n",
      " b'Transformers, composed of multiple self-attention layers, hold strong\\npromises toward a generic learning primitive applicable to different data\\nmodalities, including the recent breakthroughs in computer vision achieving\\nstate-of-the-art (SOTA) standard accuracy with better parameter efficiency.\\nSince self-attention helps a model systematically align different components\\npresent inside the input data, it leaves grounds to investigate its performance\\nunder model robustness benchmarks. In this work, we study the robustness of the\\nVision Transformer (ViT) against common corruptions and perturbations,\\ndistribution shifts, and natural adversarial examples. We use six different\\ndiverse ImageNet datasets concerning robust classification to conduct a\\ncomprehensive performance comparison of ViT models and SOTA convolutional\\nneural networks (CNNs), Big-Transfer. Through a series of six systematically\\ndesigned experiments, we then present analyses that provide both quantitative\\nand qualitative indications to explain why ViTs are indeed more robust\\nlearners. For example, with fewer parameters and similar dataset and\\npre-training combinations, ViT gives a top-1 accuracy of 28.10% on ImageNet-A\\nwhich is 4.3x higher than a comparable variant of BiT. Our analyses on image\\nmasking, Fourier spectrum sensitivity, and spread on discrete cosine energy\\nspectrum reveal intriguing properties of ViT attributing to improved\\nrobustness. Code for reproducing our experiments is available here:\\nhttps://git.io/J3VO0.'\n",
      " b'Many common sequential data sources, such as source code and natural\\nlanguage, have a natural tree-structured representation. These trees can be\\ngenerated by fitting a sequence to a grammar, yielding a hierarchical ordering\\nof the tokens in the sequence. This structure encodes a high degree of\\nsyntactic information, making it ideal for problems such as grammar correction.\\nHowever, little work has been done to develop neural networks that can operate\\non and exploit tree-structured data. In this paper we present the\\nTree-Transformer \\\\textemdash{} a novel neural network architecture designed to\\ntranslate between arbitrary input and output trees. We applied this\\narchitecture to correction tasks in both the source code and natural language\\ndomains. On source code, our model achieved an improvement of $25\\\\%$\\n$\\\\text{F}0.5$ over the best sequential method. On natural language, we achieved\\ncomparable results to the most complex state of the art systems, obtaining a\\n$10\\\\%$ improvement in recall on the CoNLL 2014 benchmark and the highest to\\ndate $\\\\text{F}0.5$ score on the AESW benchmark of $50.43$.'\n",
      " b'In this paper, we present a regression-based pose recognition method using\\ncascade Transformers. One way to categorize the existing approaches in this\\ndomain is to separate them into 1). heatmap-based and 2). regression-based. In\\ngeneral, heatmap-based methods achieve higher accuracy but are subject to\\nvarious heuristic designs (not end-to-end mostly), whereas regression-based\\napproaches attain relatively lower accuracy but they have less intermediate\\nnon-differentiable steps. Here we utilize the encoder-decoder structure in\\nTransformers to perform regression-based person and keypoint detection that is\\ngeneral-purpose and requires less heuristic design compared with the existing\\napproaches. We demonstrate the keypoint hypothesis (query) refinement process\\nacross different self-attention layers to reveal the recursive self-attention\\nmechanism in Transformers. In the experiments, we report competitive results\\nfor pose recognition when compared with the competing regression-based methods.'\n",
      " b'We investigate and improve self-supervision as a drop-in replacement for\\nImageNet pretraining, focusing on automatic colorization as the proxy task.\\nSelf-supervised training has been shown to be more promising for utilizing\\nunlabeled data than other, traditional unsupervised learning methods. We build\\non this success and evaluate the ability of our self-supervised network in\\nseveral contexts. On VOC segmentation and classification tasks, we present\\nresults that are state-of-the-art among methods not using ImageNet labels for\\npretraining representations.\\n  Moreover, we present the first in-depth analysis of self-supervision via\\ncolorization, concluding that formulation of the loss, training details and\\nnetwork architecture play important roles in its effectiveness. This\\ninvestigation is further expanded by revisiting the ImageNet pretraining\\nparadigm, asking questions such as: How much training data is needed? How many\\nlabels are needed? How much do features change when fine-tuned? We relate these\\nquestions back to self-supervision by showing that colorization provides a\\nsimilarly powerful supervisory signal as various flavors of ImageNet\\npretraining.'\n",
      " b'In preoperative imaging, the demarcation of rectal cancer with magnetic\\nresonance images provides an important basis for cancer staging and treatment\\nplanning. Recently, deep learning has greatly improved the state-of-the-art\\nmethod in automatic segmentation. However, limitations in data availability in\\nthe medical field can cause large variance and consequent overfitting to\\nmedical image segmentation networks. In this study, we propose methods to\\nreduce the model variance of a rectal cancer segmentation network by adding a\\nrectum segmentation task and performing data augmentation; the geometric\\ncorrelation between the rectum and rectal cancer motivated the former approach.\\nMoreover, we propose a method to perform a bias-variance analysis within an\\narbitrary region-of-interest (ROI) of a segmentation network, which we applied\\nto assess the efficacy of our approaches in reducing model variance. As a\\nresult, adding a rectum segmentation task reduced the model variance of the\\nrectal cancer segmentation network within tumor regions by a factor of 0.90;\\ndata augmentation further reduced the variance by a factor of 0.89. These\\napproaches also reduced the training duration by a factor of 0.96 and a further\\nfactor of 0.78, respectively. Our approaches will improve the quality of rectal\\ncancer staging by increasing the accuracy of its automatic demarcation and by\\nproviding rectum boundary information since rectal cancer staging requires the\\ndemarcation of both rectum and rectal cancer. Besides such clinical benefits,\\nour method also enables segmentation networks to be assessed with bias-variance\\nanalysis within an arbitrary ROI, such as a cancerous region.'\n",
      " b'Deep learning (DL) models for disease classification or segmentation from\\nmedical images are increasingly trained using transfer learning (TL) from\\nunrelated natural world images. However, shortcomings and utility of TL for\\nspecialized tasks in the medical imaging domain remain unknown and are based on\\nassumptions that increasing training data will improve performance. We report\\ndetailed comparisons, rigorous statistical analysis and comparisons of widely\\nused DL architecture for binary segmentation after TL with ImageNet\\ninitialization (TII-models) with supervised learning with only medical\\nimages(LMI-models) of macroscopic optical skin cancer, microscopic prostate\\ncore biopsy and Computed Tomography (CT) DICOM images. Through visual\\ninspection of TII and LMI model outputs and their Grad-CAM counterparts, our\\nresults identify several counter intuitive scenarios where automated\\nsegmentation of one tumor by both models or the use of individual segmentation\\noutput masks in various combinations from individual models leads to 10%\\nincrease in performance. We also report sophisticated ensemble DL strategies\\nfor achieving clinical grade medical image segmentation and model explanations\\nunder low data regimes. For example; estimating performance, explanations and\\nreplicability of LMI and TII models described by us can be used for situations\\nin which sparsity promotes better learning. A free GitHub repository of TII and\\nLMI models, code and more than 10,000 medical images and their Grad-CAM output\\nfrom this study can be used as starting points for advanced computational\\nmedicine and DL research for biomedical discovery and applications.'\n",
      " b'Point cloud registration is a key task in many computational fields. Previous\\ncorrespondence matching based methods require the inputs to have distinctive\\ngeometric structures to fit a 3D rigid transformation according to point-wise\\nsparse feature matches. However, the accuracy of transformation heavily relies\\non the quality of extracted features, which are prone to errors with respect to\\npartiality and noise. In addition, they can not utilize the geometric knowledge\\nof all the overlapping regions. On the other hand, previous global feature\\nbased approaches can utilize the entire point cloud for the registration,\\nhowever they ignore the negative effect of non-overlapping points when\\naggregating global features. In this paper, we present OMNet, a global feature\\nbased iterative network for partial-to-partial point cloud registration. We\\nlearn overlapping masks to reject non-overlapping regions, which converts the\\npartial-to-partial registration to the registration of the same shape.\\nMoreover, the previously used data is sampled only once from the CAD models for\\neach object, resulting in the same point clouds for the source and reference.\\nWe propose a more practical manner of data generation where a CAD model is\\nsampled twice for the source and reference, avoiding the previously prevalent\\nover-fitting issue. Experimental results show that our method achieves\\nstate-of-the-art performance compared to traditional and deep learning based\\nmethods. Code is available at https://github.com/megvii-research/OMNet.'\n",
      " b\"Generative adversarial networks (GANs) are well known for their unsupervised\\nlearning capabilities. A recent success in the field of astronomy is deblending\\ntwo overlapping galaxy images via a branched GAN model. However, it remains a\\nsignificant challenge to comprehend how the network works, which is\\nparticularly difficult for non-expert users. This research focuses on behaviors\\nof one of the network's major components, the Discriminator, which plays a\\nvital role but is often overlooked, Specifically, we enhance the Layer-wise\\nRelevance Propagation (LRP) scheme to generate a heatmap-based visualization.\\nWe call this technique Polarized-LRP and it consists of two parts i.e. positive\\ncontribution heatmaps for ground truth images and negative contribution\\nheatmaps for generated images. Using the Galaxy Zoo dataset we demonstrate that\\nour method clearly reveals attention areas of the Discriminator when\\ndifferentiating generated galaxy images from ground truth images. To connect\\nthe Discriminator's impact on the Generator, we visualize the gradual changes\\nof the Generator across the training process. An interesting result we have\\nachieved there is the detection of a problematic data augmentation procedure\\nthat would else have remained hidden. We find that our proposed method serves\\nas a useful visual analytical tool for a deeper understanding of GAN models.\"\n",
      " b'Graph Neural Networks (GNNs) have achieved tremendous success in various\\nreal-world applications due to their strong ability in graph representation\\nlearning. GNNs explore the graph structure and node features by aggregating and\\ntransforming information within node neighborhoods. However, through\\ntheoretical and empirical analysis, we reveal that the aggregation process of\\nGNNs tends to destroy node similarity in the original feature space. There are\\nmany scenarios where node similarity plays a crucial role. Thus, it has\\nmotivated the proposed framework SimP-GCN that can effectively and efficiently\\npreserve node similarity while exploiting graph structure. Specifically, to\\nbalance information from graph structure and node features, we propose a\\nfeature similarity preserving aggregation which adaptively integrates graph\\nstructure and node features. Furthermore, we employ self-supervised learning to\\nexplicitly capture the complex feature similarity and dissimilarity relations\\nbetween nodes. We validate the effectiveness of SimP-GCN on seven benchmark\\ndatasets including three assortative and four disassorative graphs. The results\\ndemonstrate that SimP-GCN outperforms representative baselines. Further probe\\nshows various advantages of the proposed framework. The implementation of\\nSimP-GCN is available at \\\\url{https://github.com/ChandlerBang/SimP-GCN}.'\n",
      " b'Training generative adversarial networks (GANs) on high quality (HQ) images\\ninvolves important computing resources. This requirement represents a\\nbottleneck for the development of applications of GANs. We propose a transfer\\nlearning technique for GANs that significantly reduces training time. Our\\napproach consists of freezing the low-level layers of both the critic and\\ngenerator of the original GAN. We assume an autoencoder constraint in order to\\nensure the compatibility of the internal representations of the critic and the\\ngenerator. This assumption explains the gain in training time as it enables us\\nto bypass the low-level layers during the forward and backward passes. We\\ncompare our method to baselines and observe a significant acceleration of the\\ntraining. It can reach two orders of magnitude on HQ datasets when compared\\nwith StyleGAN. We prove rigorously, within the framework of optimal transport,\\na theorem ensuring the convergence of the learning of the transferred GAN. We\\nmoreover provide a precise bound for the convergence of the training in terms\\nof the distance between the source and target dataset.'\n",
      " b\"Machine learning plays an increasingly significant role in many aspects of\\nour lives (including medicine, transportation, security, justice and other\\ndomains), making the potential consequences of false predictions increasingly\\ndevastating. These consequences may be mitigated if we can automatically flag\\nsuch false predictions and potentially assign them to alternative, more\\nreliable mechanisms, that are possibly more costly and involve human attention.\\nThis suggests the task of detecting errors, which we tackle in this paper for\\nthe case of visual classification. To this end, we propose a novel approach for\\nclassification confidence estimation. We apply a set of semantics-preserving\\nimage transformations to the input image, and show how the resulting image sets\\ncan be used to estimate confidence in the classifier's prediction. We\\ndemonstrate the potential of our approach by extensively evaluating it on a\\nwide variety of classifier architectures and datasets, including\\nResNext/ImageNet, achieving state of the art performance. This paper\\nconstitutes a significant revision of our earlier work in this direction (Bahat\\n& Shakhnarovich, 2018).\"\n",
      " b'Machine learning has been utilized to perform tasks in many different domains\\nsuch as classification, object detection, image segmentation and natural\\nlanguage analysis. Data labeling has always been one of the most important\\ntasks in machine learning. However, labeling large amounts of data increases\\nthe monetary cost in machine learning. As a result, researchers started to\\nfocus on reducing data annotation and labeling costs. Transfer learning was\\ndesigned and widely used as an efficient approach that can reasonably reduce\\nthe negative impact of limited data, which in turn, reduces the data\\npreparation cost. Even transferring previous knowledge from a source domain\\nreduces the amount of data needed in a target domain. However, large amounts of\\nannotated data are still demanded to build robust models and improve the\\nprediction accuracy of the model. Therefore, researchers started to pay more\\nattention on auto annotation and labeling. In this survey paper, we provide a\\nreview of previous techniques that focuses on optimized data annotation and\\nlabeling for video, audio, and text data.'\n",
      " b'A strong visual object tracker nowadays relies on its well-crafted modules,\\nwhich typically consist of manually-designed network architectures to deliver\\nhigh-quality tracking results. Not surprisingly, the manual design process\\nbecomes a particularly challenging barrier, as it demands sufficient prior\\nexperience, enormous effort, intuition and perhaps some good luck. Meanwhile,\\nneural architecture search has gaining grounds in practical applications such\\nas image segmentation, as a promising method in tackling the issue of automated\\nsearch of feasible network structures. In this work, we propose a novel\\ncell-level differentiable architecture search mechanism to automate the network\\ndesign of the tracking module, aiming to adapt backbone features to the\\nobjective of a tracking network during offline training. The proposed approach\\nis simple, efficient, and with no need to stack a series of modules to\\nconstruct a network. Our approach is easy to be incorporated into existing\\ntrackers, which is empirically validated using different differentiable\\narchitecture search-based methods and tracking objectives. Extensive\\nexperimental evaluations demonstrate the superior performance of our approach\\nover five commonly-used benchmarks. Meanwhile, our automated searching process\\ntakes 41 (18) hours for the second (first) order DARTS method on the\\nTrackingNet dataset.'\n",
      " b'We explore the task of Video Object Grounding (VOG), which grounds objects in\\nvideos referred to in natural language descriptions. Previous methods apply\\nimage grounding based algorithms to address VOG, fail to explore the object\\nrelation information and suffer from limited generalization. Here, we\\ninvestigate the role of object relations in VOG and propose a novel framework\\nVOGNet to encode multi-modal object relations via self-attention with relative\\nposition encoding. To evaluate VOGNet, we propose novel contrasting sampling\\nmethods to generate more challenging grounding input samples, and construct a\\nnew dataset called ActivityNet-SRL (ASRL) based on existing caption and\\ngrounding datasets. Experiments on ASRL validate the need of encoding object\\nrelations in VOG, and our VOGNet outperforms competitive baselines by a\\nsignificant margin.'\n",
      " b'Despite the recent active research on processing point clouds with deep\\nnetworks, few attention has been on the sensitivity of the networks to\\nrotations. In this paper, we propose a deep learning architecture that achieves\\ndiscrete $\\\\mathbf{SO}(2)$/$\\\\mathbf{SO}(3)$ rotation equivariance for point\\ncloud recognition. Specifically, the rotation of an input point cloud with\\nelements of a rotation group is similar to shuffling the feature vectors\\ngenerated by our approach. The equivariance is easily reduced to invariance by\\neliminating the permutation with operations such as maximum or average. Our\\nmethod can be directly applied to any existing point cloud based networks,\\nresulting in significant improvements in their performance for rotated inputs.\\nWe show state-of-the-art results in the classification tasks with various\\ndatasets under both $\\\\mathbf{SO}(2)$ and $\\\\mathbf{SO}(3)$ rotations. In\\naddition, we further analyze the necessary conditions of applying our approach\\nto PointNet based networks. Source codes at\\nhttps://github.com/lijx10/rot-equ-net'\n",
      " b'Autonomous vehicles may make wrong decisions due to inaccurate detection and\\nrecognition. Therefore, an intelligent vehicle can combine its own data with\\nthat of other vehicles to enhance perceptive ability, and thus improve\\ndetection accuracy and driving safety. However, multi-vehicle cooperative\\nperception requires the integration of real world scenes and the traffic of raw\\nsensor data exchange far exceeds the bandwidth of existing vehicular networks.\\nTo the best our knowledge, we are the first to conduct a study on raw-data\\nlevel cooperative perception for enhancing the detection ability of\\nself-driving systems. In this work, relying on LiDAR 3D point clouds, we fuse\\nthe sensor data collected from different positions and angles of connected\\nvehicles. A point cloud based 3D object detection method is proposed to work on\\na diversity of aligned point clouds. Experimental results on KITTI and our\\ncollected dataset show that the proposed system outperforms perception by\\nextending sensing area, improving detection accuracy and promoting augmented\\nresults. Most importantly, we demonstrate it is possible to transmit point\\nclouds data for cooperative perception via existing vehicular network\\ntechnologies.'\n",
      " b'We propose a novel method for automatic reasoning on knowledge graphs based\\non debate dynamics. The main idea is to frame the task of triple classification\\nas a debate game between two reinforcement learning agents which extract\\narguments -- paths in the knowledge graph -- with the goal to promote the fact\\nbeing true (thesis) or the fact being false (antithesis), respectively. Based\\non these arguments, a binary classifier, called the judge, decides whether the\\nfact is true or false. The two agents can be considered as sparse, adversarial\\nfeature generators that present interpretable evidence for either the thesis or\\nthe antithesis. In contrast to other black-box methods, the arguments allow\\nusers to get an understanding of the decision of the judge. Since the focus of\\nthis work is to create an explainable method that maintains a competitive\\npredictive accuracy, we benchmark our method on the triple classification and\\nlink prediction task. Thereby, we find that our method outperforms several\\nbaselines on the benchmark datasets FB15k-237, WN18RR, and Hetionet. We also\\nconduct a survey and find that the extracted arguments are informative for\\nusers.'\n",
      " b'The field of meta-learning, or learning-to-learn, has seen a dramatic rise in\\ninterest in recent years. Contrary to conventional approaches to AI where tasks\\nare solved from scratch using a fixed learning algorithm, meta-learning aims to\\nimprove the learning algorithm itself, given the experience of multiple\\nlearning episodes. This paradigm provides an opportunity to tackle many\\nconventional challenges of deep learning, including data and computation\\nbottlenecks, as well as generalization. This survey describes the contemporary\\nmeta-learning landscape. We first discuss definitions of meta-learning and\\nposition it with respect to related fields, such as transfer learning and\\nhyperparameter optimization. We then propose a new taxonomy that provides a\\nmore comprehensive breakdown of the space of meta-learning methods today. We\\nsurvey promising applications and successes of meta-learning such as few-shot\\nlearning and reinforcement learning. Finally, we discuss outstanding challenges\\nand promising areas for future research.'\n",
      " b'Many vision-language tasks can be reduced to the problem of sequence\\nprediction for natural language output. In particular, recent advances in image\\ncaptioning use deep reinforcement learning (RL) to alleviate the \"exposure\\nbias\" during training: ground-truth subsequence is exposed in every step\\nprediction, which introduces bias in test when only predicted subsequence is\\nseen. However, existing RL-based image captioning methods only focus on the\\nlanguage policy while not the visual policy (e.g., visual attention), and thus\\nfail to capture the visual context that are crucial for compositional reasoning\\nsuch as visual relationships (e.g., \"man riding horse\") and comparisons (e.g.,\\n\"smaller cat\"). To fill the gap, we propose a Context-Aware Visual Policy\\nnetwork (CAVP) for sequence-level image captioning. At every time step, CAVP\\nexplicitly accounts for the previous visual attentions as the context, and then\\ndecides whether the context is helpful for the current word generation given\\nthe current visual attention. Compared against traditional visual attention\\nthat only fixes a single image region at every step, CAVP can attend to complex\\nvisual compositions over time. The whole image captioning model --- CAVP and\\nits subsequent language policy network --- can be efficiently optimized\\nend-to-end by using an actor-critic policy gradient method with respect to any\\ncaption evaluation metric. We demonstrate the effectiveness of CAVP by\\nstate-of-the-art performances on MS-COCO offline split and online server, using\\nvarious metrics and sensible visualizations of qualitative visual context. The\\ncode is available at https://github.com/daqingliu/CAVP'\n",
      " b'Recent advances in generative adversarial networks (GANs) have shown great\\npotentials in realistic image synthesis whereas most existing works address\\nsynthesis realism in either appearance space or geometry space but few in both.\\nThis paper presents an innovative Spatial Fusion GAN (SF-GAN) that combines a\\ngeometry synthesizer and an appearance synthesizer to achieve synthesis realism\\nin both geometry and appearance spaces. The geometry synthesizer learns\\ncontextual geometries of background images and transforms and places foreground\\nobjects into the background images unanimously. The appearance synthesizer\\nadjusts the color, brightness and styles of the foreground objects and embeds\\nthem into background images harmoniously, where a guided filter is introduced\\nfor detail preserving. The two synthesizers are inter-connected as mutual\\nreferences which can be trained end-to-end without supervision. The SF-GAN has\\nbeen evaluated in two tasks: (1) realistic scene text image synthesis for\\ntraining better recognition models; (2) glass and hat wearing for realistic\\nmatching glasses and hats with real portraits. Qualitative and quantitative\\ncomparisons with the state-of-the-art demonstrate the superiority of the\\nproposed SF-GAN.'\n",
      " b'Random fields have remained a topic of great interest over past decades for\\nthe purpose of structured inference, especially for problems such as image\\nsegmentation. The local nodal interactions commonly used in such models often\\nsuffer the short-boundary bias problem, which are tackled primarily through the\\nincorporation of long-range nodal interactions. However, the issue of\\ncomputational tractability becomes a significant issue when incorporating such\\nlong-range nodal interactions, particularly when a large number of long-range\\nnodal interactions (e.g., fully-connected random fields) are modeled.\\n  In this work, we introduce a generalized random field framework based around\\nthe concept of stochastic cliques, which addresses the issue of computational\\ntractability when using fully-connected random fields by stochastically forming\\na sparse representation of the random field. The proposed framework allows for\\nefficient structured inference using fully-connected random fields without any\\nrestrictions on the potential functions that can be utilized. Several\\nrealizations of the proposed framework using graph cuts are presented and\\nevaluated, and experimental results demonstrate that the proposed framework can\\nprovide competitive performance for the purpose of image segmentation when\\ncompared to existing fully-connected and principled deep random field\\nframeworks.'\n",
      " b'While generative models have shown great success in generating\\nhigh-dimensional samples conditional on low-dimensional descriptors (learning\\ne.g. stroke thickness in MNIST, hair color in CelebA, or speaker identity in\\nWavenet), their generation out-of-sample poses fundamental problems. The\\nconditional variational autoencoder (CVAE) as a simple conditional generative\\nmodel does not explicitly relate conditions during training and, hence, has no\\nincentive of learning a compact joint distribution across conditions. We\\novercome this limitation by matching their distributions using maximum mean\\ndiscrepancy (MMD) in the decoder layer that follows the bottleneck. This\\nintroduces a strong regularization both for reconstructing samples within the\\nsame condition and for transforming samples across conditions, resulting in\\nmuch improved generalization. We refer to the architecture as\\n\\\\emph{transformer} VAE (trVAE). Benchmarking trVAE on high-dimensional image\\nand tabular data, we demonstrate higher robustness and higher accuracy than\\nexisting approaches. In particular, we show qualitatively improved predictions\\nfor cellular perturbation response to treatment and disease based on\\nhigh-dimensional single-cell gene expression data, by tackling previously\\nproblematic minority classes and multiple conditions. For generic tasks, we\\nimprove Pearson correlations of high-dimensional estimated means and variances\\nwith their ground truths from 0.89 to 0.97 and 0.75 to 0.87, respectively.'\n",
      " b'Referring image segmentation aims at segmenting the foreground masks of the\\nentities that can well match the description given in the natural language\\nexpression. Previous approaches tackle this problem using implicit feature\\ninteraction and fusion between visual and linguistic modalities, but usually\\nfail to explore informative words of the expression to well align features from\\nthe two modalities for accurately identifying the referred entity. In this\\npaper, we propose a Cross-Modal Progressive Comprehension (CMPC) module and a\\nText-Guided Feature Exchange (TGFE) module to effectively address the\\nchallenging task. Concretely, the CMPC module first employs entity and\\nattribute words to perceive all the related entities that might be considered\\nby the expression. Then, the relational words are adopted to highlight the\\ncorrect entity as well as suppress other irrelevant ones by multimodal graph\\nreasoning. In addition to the CMPC module, we further leverage a simple yet\\neffective TGFE module to integrate the reasoned multimodal features from\\ndifferent levels with the guidance of textual information. In this way,\\nfeatures from multi-levels could communicate with each other and be refined\\nbased on the textual context. We conduct extensive experiments on four popular\\nreferring segmentation benchmarks and achieve new state-of-the-art\\nperformances.'\n",
      " b'Moving objects have special importance for Autonomous Driving tasks.\\nDetecting moving objects can be posed as Moving Object Segmentation, by\\nsegmenting the object pixels, or Moving Object Detection, by generating a\\nbounding box for the moving targets. In this paper, we present a Multi-Task\\nLearning architecture, based on Transformers, to jointly perform both tasks\\nthrough one network. Due to the importance of the motion features to the task,\\nthe whole setup is based on a Spatio-Temporal aggregation. We evaluate the\\nperformance of the individual tasks architecture versus the MTL setup, both\\nwith early shared encoders, and late shared encoder-decoder transformers. For\\nthe latter, we present a novel joint tasks query decoder transformer, that\\nenables us to have tasks dedicated heads out of the shared model. To evaluate\\nour approach, we use the KITTI MOD [29] data set. Results show1.5% mAP\\nimprovement for Moving Object Detection, and 2%IoU improvement for Moving\\nObject Segmentation, over the individual tasks networks.'\n",
      " b\"Detection of moving objects is a very important task in autonomous driving\\nsystems. After the perception phase, motion planning is typically performed in\\nBird's Eye View (BEV) space. This would require projection of objects detected\\non the image plane to top view BEV plane. Such a projection is prone to errors\\ndue to lack of depth information and noisy mapping in far away areas. CNNs can\\nleverage the global context in the scene to project better. In this work, we\\nexplore end-to-end Moving Object Detection (MOD) on the BEV map directly using\\nmonocular images as input. To the best of our knowledge, such a dataset does\\nnot exist and we create an extended KITTI-raw dataset consisting of 12.9k\\nimages with annotations of moving object masks in BEV space for five classes.\\nThe dataset is intended to be used for class agnostic motion cue based object\\ndetection and classes are provided as meta-data for better tuning. We design\\nand implement a two-stream RGB and optical flow fusion architecture which\\noutputs motion segmentation directly in BEV space. We compare it with inverse\\nperspective mapping of state-of-the-art motion segmentation predictions on the\\nimage plane. We observe a significant improvement of 13% in mIoU using the\\nsimple baseline implementation. This demonstrates the ability to directly learn\\nmotion segmentation output in BEV space. Qualitative results of our baseline\\nand the dataset annotations can be found in\\nhttps://sites.google.com/view/bev-modnet.\"\n",
      " b'Scarcity of high quality annotated images remains a limiting factor for\\ntraining accurate image segmentation models. While more and more annotated\\ndatasets become publicly available, the number of samples in each individual\\ndatabase is often small. Combining different databases to create larger amounts\\nof training data is appealing yet challenging due to the heterogeneity as a\\nresult of differences in data acquisition and annotation processes, often\\nyielding incompatible or even conflicting information. In this paper, we\\ninvestigate and propose several strategies for learning from partially\\noverlapping labels in the context of abdominal organ segmentation. We find that\\ncombining a semi-supervised approach with an adaptive cross entropy loss can\\nsuccessfully exploit heterogeneously annotated data and substantially improve\\nsegmentation accuracy compared to baseline and alternative approaches.'\n",
      " b'This letter presents a novel framework termed DistSTN for the task of\\nsynthetic aperture radar (SAR) automatic target recognition (ATR). In contrast\\nto the conventional SAR ATR algorithms, DistSTN considers a more challenging\\npractical scenario for non-cooperative targets whose aspect angles for training\\nare incomplete and limited in a partial range while those of testing samples\\nare unlimited. To address this issue, instead of learning the pose invariant\\nfeatures, DistSTN newly involves an elaborated feature disentangling model to\\nseparate the learned pose factors of a SAR target from the identity ones so\\nthat they can independently control the representation process of the target\\nimage. To disentangle the explainable pose factors, we develop a pose\\ndiscrepancy spatial transformer module in DistSTN to characterize the intrinsic\\ntransformation between the factors of two different targets with an explicit\\ngeometric model. Furthermore, DistSTN develops an amortized inference scheme\\nthat enables efficient feature extraction and recognition using an\\nencoder-decoder mechanism. Experimental results with the moving and stationary\\ntarget acquisition and recognition (MSTAR) benchmark demonstrate the\\neffectiveness of our proposed approach. Compared with the other ATR algorithms,\\nDistSTN can achieve higher recognition accuracy.'\n",
      " b'Generative Adversarial Networks (GAN) are known to produce synthetic data\\nthat are difficult to discern from real ones by humans. In this paper we\\npresent an approach to use GAN to produce realistically looking ECG signals. We\\nutilize them to train and evaluate a denoising autoencoder that achieves\\nstate-of-the-art filtering quality for ECG signals. It is demonstrated that\\ngenerated data improves the model performance compared to the model trained on\\nreal data only. We also investigate an effect of transfer learning by reusing\\ntrained discriminator network for denoising model.'\n",
      " b'Feature pyramids and iterative refinement have recently led to great progress\\nin optical flow estimation. However, downsampling in feature pyramids can cause\\nblending of foreground objects with the background, which will mislead\\nsubsequent decisions in the iterative processing. The results are missing\\ndetails especially in the flow of thin and of small structures. We propose a\\nnovel Residual Feature Pyramid Module (RFPM) which retains important details in\\nthe feature map without changing the overall iterative refinement design of the\\noptical flow estimation. RFPM incorporates a residual structure between\\nmultiple feature pyramids into a downsampling module that corrects the blending\\nof objects across boundaries. We demonstrate how to integrate our module with\\ntwo state-of-the-art iterative refinement architectures. Results show that our\\nRFPM visibly reduces flow errors and improves state-of-art performance in the\\nclean pass of Sintel, and is one of the top-performing methods in KITTI.\\nAccording to the particular modular structure of RFPM, we introduce a special\\ntransfer learning approach that can dramatically decrease the training time\\ncompared to a typical full optical flow training schedule on multiple datasets.'\n",
      " b'In this paper, we propose a novel form of the loss function to increase the\\nperformance of LiDAR-based 3d object detection and obtain more explainable and\\nconvincing uncertainty for the prediction. The loss function was designed using\\ncorner transformation and uncertainty modeling. With the new loss function, the\\nperformance of our method on the val split of KITTI dataset shows up to a 15%\\nincrease in terms of Average Precision (AP) comparing with the baseline using\\nsimple L1 Loss. In the study of the characteristics of predicted uncertainties,\\nwe find that generally more accurate prediction of the bounding box is usually\\naccompanied by lower uncertainty. The distribution of corner uncertainties\\nagrees on the distribution of the point cloud in the bounding box, which means\\nthe corner with denser observed points has lower uncertainty. Moreover, our\\nmethod also learns the constraint from the cuboid geometry of the bounding box\\nin uncertainty prediction. Finally, we propose an efficient Bayesian updating\\nmethod to recover the uncertainty for the original parameters of the bounding\\nboxes which can help to provide probabilistic results for the planning module.'\n",
      " b'Objective: Herein, a neural network-based liver segmentation algorithm is\\nproposed, and its performance was evaluated using abdominal computed tomography\\n(CT) images. Methods: A fully convolutional network was developed to overcome\\nthe volumetric image segmentation problem. To guide a neural network to\\naccurately delineate a target liver object, the network was deeply supervised\\nby applying the adaptive self-supervision scheme to derive the essential\\ncontour, which acted as a complement with the global shape. The discriminative\\ncontour, shape, and deep features were internally merged for the segmentation\\nresults. Results and Conclusion: 160 abdominal CT images were used for training\\nand validation. The quantitative evaluation of the proposed network was\\nperformed through an eight-fold cross-validation. The result showed that the\\nmethod, which uses the contour feature, segmented the liver more accurately\\nthan the state-of-the-art with a 2.13% improvement in the dice score.\\nSignificance: In this study, a new framework was introduced to guide a neural\\nnetwork and learn complementary contour features. The proposed neural network\\ndemonstrates that the guided contour features can significantly improve the\\nperformance of the segmentation task.'\n",
      " b'The ability to perform effective planning is crucial for building an\\ninstruction-following agent. When navigating through a new environment, an\\nagent is challenged with (1) connecting the natural language instructions with\\nits progressively growing knowledge of the world; and (2) performing long-range\\nplanning and decision making in the form of effective exploration and error\\ncorrection. Current methods are still limited on both fronts despite extensive\\nefforts. In this paper, we introduce the Evolving Graphical Planner (EGP), a\\nmodel that performs global planning for navigation based on raw sensory input.\\nThe model dynamically constructs a graphical representation, generalizes the\\naction space to allow for more flexible decision making, and performs efficient\\nplanning on a proxy graph representation. We evaluate our model on a\\nchallenging Vision-and-Language Navigation (VLN) task with photorealistic\\nimages and achieve superior performance compared to previous navigation\\narchitectures. For instance, we achieve a 53% success rate on the test split of\\nthe Room-to-Room navigation task through pure imitation learning, outperforming\\nprevious navigation architectures by up to 5%.'\n",
      " b\"Scarcity of labeled data has motivated the development of semi-supervised\\nlearning methods, which learn from large portions of unlabeled data alongside a\\nfew labeled samples. Consistency Regularization between model's predictions\\nunder different input perturbations, particularly has shown to provide\\nstate-of-the art results in a semi-supervised framework. However, most of these\\nmethod have been limited to classification and segmentation applications. We\\npropose Transformation Consistency Regularization, which delves into a more\\nchallenging setting of image-to-image translation, which remains unexplored by\\nsemi-supervised algorithms. The method introduces a diverse set of geometric\\ntransformations and enforces the model's predictions for unlabeled data to be\\ninvariant to those transformations. We evaluate the efficacy of our algorithm\\non three different applications: image colorization, denoising and\\nsuper-resolution. Our method is significantly data efficient, requiring only\\naround 10 - 20% of labeled samples to achieve similar image reconstructions to\\nits fully-supervised counterpart. Furthermore, we show the effectiveness of our\\nmethod in video processing applications, where knowledge from a few frames can\\nbe leveraged to enhance the quality of the rest of the movie.\"\n",
      " b'Existing vision-based action recognition is susceptible to occlusion and\\nappearance variations, while wearable sensors can alleviate these challenges by\\ncapturing human motion with one-dimensional time-series signal. For the same\\naction, the knowledge learned from vision sensors and wearable sensors, may be\\nrelated and complementary. However, there exists significantly large modality\\ndifference between action data captured by wearable-sensor and vision-sensor in\\ndata dimension, data distribution and inherent information content. In this\\npaper, we propose a novel framework, named Semantics-aware Adaptive Knowledge\\nDistillation Networks (SAKDN), to enhance action recognition in vision-sensor\\nmodality (videos) by adaptively transferring and distilling the knowledge from\\nmultiple wearable sensors. The SAKDN uses multiple wearable-sensors as teacher\\nmodalities and uses RGB videos as student modality. To preserve local temporal\\nrelationship and facilitate employing visual deep learning model, we transform\\none-dimensional time-series signals of wearable sensors to two-dimensional\\nimages by designing a gramian angular field based virtual image generation\\nmodel. Then, we build a novel Similarity-Preserving Adaptive Multi-modal Fusion\\nModule to adaptively fuse intermediate representation knowledge from different\\nteacher networks. Finally, to fully exploit and transfer the knowledge of\\nmultiple well-trained teacher networks to the student network, we propose a\\nnovel Graph-guided Semantically Discriminative Mapping loss, which utilizes\\ngraph-guided ablation analysis to produce a good visual explanation\\nhighlighting the important regions across modalities and concurrently\\npreserving the interrelations of original data. Experimental results on\\nBerkeley-MHAD, UTD-MHAD and MMAct datasets well demonstrate the effectiveness\\nof our proposed SAKDN.'\n",
      " b'We present an approach to infer the 3D shape, texture, and camera pose for an\\nobject from a single RGB image, using only category-level image collections\\nwith foreground masks as supervision. We represent the shape as an\\nimage-conditioned implicit function that transforms the surface of a sphere to\\nthat of the predicted mesh, while additionally predicting the corresponding\\ntexture. To derive supervisory signal for learning, we enforce that: a) our\\npredictions when rendered should explain the available image evidence, and b)\\nthe inferred 3D structure should be geometrically consistent with learned pixel\\nto surface mappings. We empirically show that our approach improves over prior\\nwork that leverages similar supervision, and in fact performs competitively to\\nmethods that use stronger supervision. Finally, as our method enables learning\\nwith limited supervision, we qualitatively demonstrate its applicability over a\\nset of about 30 object categories.'\n",
      " b'Tabular datasets are ubiquitous in data science applications. Given their\\nimportance, it seems natural to apply state-of-the-art deep learning algorithms\\nin order to fully unlock their potential. Here we propose neural network models\\nthat represent tabular time series that can optionally leverage their\\nhierarchical structure. This results in two architectures for tabular time\\nseries: one for learning representations that is analogous to BERT and can be\\npre-trained end-to-end and used in downstream tasks, and one that is akin to\\nGPT and can be used for generation of realistic synthetic tabular sequences. We\\ndemonstrate our models on two datasets: a synthetic credit card transaction\\ndataset, where the learned representations are used for fraud detection and\\nsynthetic data generation, and on a real pollution dataset, where the learned\\nencodings are used to predict atmospheric pollutant concentrations. Code and\\ndata are available at https://github.com/IBM/TabFormer.'\n",
      " b'Video captioning aims to automatically generate natural language descriptions\\nof video content, which has drawn a lot of attention recent years. Generating\\naccurate and fine-grained captions needs to not only understand the global\\ncontent of video, but also capture the detailed object information. Meanwhile,\\nvideo representations have great impact on the quality of generated captions.\\nThus, it is important for video captioning to capture salient objects with\\ntheir detailed temporal dynamics, and represent them using discriminative\\nspatio-temporal representations. In this paper, we propose a new video\\ncaptioning approach based on object-aware aggregation with bidirectional\\ntemporal graph (OA-BTG), which captures detailed temporal dynamics for salient\\nobjects in video, and learns discriminative spatio-temporal representations by\\nperforming object-aware local feature aggregation on detected object regions.\\nThe main novelties and advantages are: (1) Bidirectional temporal graph: A\\nbidirectional temporal graph is constructed along and reversely along the\\ntemporal order, which provides complementary ways to capture the temporal\\ntrajectories for each salient object. (2) Object-aware aggregation: Learnable\\nVLAD (Vector of Locally Aggregated Descriptors) models are constructed on\\nobject temporal trajectories and global frame sequence, which performs\\nobject-aware aggregation to learn discriminative representations. A\\nhierarchical attention mechanism is also developed to distinguish different\\ncontributions of multiple objects. Experiments on two widely-used datasets\\ndemonstrate our OA-BTG achieves state-of-the-art performance in terms of\\nBLEU@4, METEOR and CIDEr metrics.'\n",
      " b'The collection of high-resolution training data is crucial in building robust\\nplant disease diagnosis systems, since such data have a significant impact on\\ndiagnostic performance. However, they are very difficult to obtain and are not\\nalways available in practice. Deep learning-based techniques, and particularly\\ngenerative adversarial networks (GANs), can be applied to generate high-quality\\nsuper-resolution images, but these methods often produce unexpected artifacts\\nthat can lower the diagnostic performance. In this paper, we propose a novel\\nartifact-suppression super-resolution method that is specifically designed for\\ndiagnosing leaf disease, called Leaf Artifact-Suppression Super Resolution\\n(LASSR). Thanks to its own artifact removal module that detects and suppresses\\nartifacts to a considerable extent, LASSR can generate much more pleasing,\\nhigh-quality images compared to the state-of-the-art ESRGAN model. Experiments\\nbased on a five-class cucumber disease (including healthy) discrimination model\\nshow that training with data generated by LASSR significantly boosts the\\nperformance on an unseen test dataset by nearly 22% compared with the baseline,\\nand that our approach is more than 2% better than a model trained with images\\ngenerated by ESRGAN.'\n",
      " b'Though image-to-sequence generation models have become overwhelmingly popular\\nin human-computer communications, they suffer from strongly favoring safe\\ngeneric questions (\"What is in this picture?\"). Generating uninformative but\\nrelevant questions is not sufficient or useful. We argue that a good question\\nis one that has a tightly focused purpose --- one that is aimed at expecting a\\nspecific type of response. We build a model that maximizes mutual information\\nbetween the image, the expected answer and the generated question. To overcome\\nthe non-differentiability of discrete natural language tokens, we introduce a\\nvariational continuous latent space onto which the expected answers project. We\\nregularize this latent space with a second latent space that ensures clustering\\nof similar answers. Even when we don\\'t know the expected answer, this second\\nlatent space can generate goal-driven questions specifically aimed at\\nextracting objects (\"what is the person throwing\"), attributes, (\"What kind of\\nshirt is the person wearing?\"), color (\"what color is the frisbee?\"), material\\n(\"What material is the frisbee?\"), etc. We quantitatively show that our model\\nis able to retain information about an expected answer category, resulting in\\nmore diverse, goal-driven questions. We launch our model on a set of real world\\nimages and extract previously unseen visual concepts.'\n",
      " b'Graph-structured data arise in a variety of real-world context ranging from\\nsensor and transportation to biological and social networks. As a ubiquitous\\ntool to process graph-structured data, spectral graph filters have been used to\\nsolve common tasks such as denoising and anomaly detection, as well as design\\ndeep learning architectures such as graph neural networks. Despite being an\\nimportant tool, there is a lack of theoretical understanding of the stability\\nproperties of spectral graph filters, which are important for designing robust\\nmachine learning models. In this paper, we study filter stability and provide a\\nnovel and interpretable upper bound on the change of filter output, where the\\nbound is expressed in terms of the endpoint degrees of the deleted and newly\\nadded edges, as well as the spatial proximity of those edges. This upper bound\\nallows us to reason, in terms of structural properties of the graph, when a\\nspectral graph filter will be stable. We further perform extensive experiments\\nto verify intuition that can be gained from the bound.'\n",
      " b'Deep neural networks have recently thrived on single image depth estimation.\\nThat being said, current developments on this topic highlight an apparent\\ncompromise between accuracy and network size. This work proposes an accurate\\nand lightweight framework for monocular depth estimation based on a\\nself-attention mechanism stemming from salient point detection. Specifically,\\nwe utilize a sparse set of keypoints to train a FuSaNet model that consists of\\ntwo major components: Fusion-Net and Saliency-Net. In addition, we introduce a\\nnormalized Hessian loss term invariant to scaling and shear along the depth\\ndirection, which is shown to substantially improve the accuracy. The proposed\\nmethod achieves state-of-the-art results on NYU-Depth-v2 and KITTI while using\\n3.1-38.4 times smaller model in terms of the number of parameters than baseline\\napproaches. Experiments on the SUN-RGBD further demonstrate the\\ngeneralizability of the proposed method.'\n",
      " b'Facial sketches drawn by artists are widely used for visual identification\\napplications and mostly by law enforcement agencies, but the quality of these\\nsketches depend on the ability of the artist to clearly replicate all the key\\nfacial features that could aid in capturing the true identity of a subject.\\nRecent works have attempted to synthesize these sketches into plausible visual\\nimages to improve visual recognition and identification. However, synthesizing\\nphoto-realistic images from sketches proves to be an even more challenging\\ntask, especially for sensitive applications such as suspect identification. In\\nthis work, we propose a novel approach that adopts a generative adversarial\\nnetwork that synthesizes a single sketch into multiple synthetic images with\\nunique attributes like hair color, sex, etc. We incorporate a hybrid\\ndiscriminator which performs attribute classification of multiple target\\nattributes, a quality guided encoder that minimizes the perceptual\\ndissimilarity of the latent space embedding of the synthesized and real image\\nat different layers in the network and an identity preserving network that\\nmaintains the identity of the synthesised image throughout the training\\nprocess. Our approach is aimed at improving the visual appeal of the\\nsynthesised images while incorporating multiple attribute assignment to the\\ngenerator without compromising the identity of the synthesised image. We\\nsynthesised sketches using XDOG filter for the CelebA, WVU Multi-modal and\\nCelebA-HQ datasets and from an auxiliary generator trained on sketches from\\nCUHK, IIT-D and FERET datasets. Our results are impressive compared to current\\nstate of the art.'\n",
      " b'Image fusion helps in merging two or more images to construct a more\\ninformative single fused image. Recently, unsupervised learning based\\nconvolutional neural networks (CNN) have been utilized for different types of\\nimage fusion tasks such as medical image fusion, infrared-visible image fusion\\nfor autonomous driving as well as multi-focus and multi-exposure image fusion\\nfor satellite imagery. However, it is challenging to analyze the reliability of\\nthese CNNs for the image fusion tasks since no groundtruth is available. This\\nled to the use of a wide variety of model architectures and optimization\\nfunctions yielding quite different fusion results. Additionally, due to the\\nhighly opaque nature of such neural networks, it is difficult to explain the\\ninternal mechanics behind its fusion results. To overcome these challenges, we\\npresent a novel real-time visualization tool, named FuseVis, with which the\\nend-user can compute per-pixel saliency maps that examine the influence of the\\ninput image pixels on each pixel of the fused image. We trained several image\\nfusion based CNNs on medical image pairs and then using our FuseVis tool, we\\nperformed case studies on a specific clinical application by interpreting the\\nsaliency maps from each of the fusion methods. We specifically visualized the\\nrelative influence of each input image on the predictions of the fused image\\nand showed that some of the evaluated image fusion methods are better suited\\nfor the specific clinical application. To the best of our knowledge, currently,\\nthere is no approach for visual analysis of neural networks for image fusion.\\nTherefore, this work opens up a new research direction to improve the\\ninterpretability of deep fusion networks. The FuseVis tool can also be adapted\\nin other deep neural network based image processing applications to make them\\ninterpretable.'\n",
      " b'Active vision is inherently attention-driven: The agent actively selects\\nviews to attend in order to fast achieve the vision task while improving its\\ninternal representation of the scene being observed. Inspired by the recent\\nsuccess of attention-based models in 2D vision tasks based on single RGB\\nimages, we propose to address the multi-view depth-based active object\\nrecognition using attention mechanism, through developing an end-to-end\\nrecurrent 3D attentional network. The architecture takes advantage of a\\nrecurrent neural network (RNN) to store and update an internal representation.\\nOur model, trained with 3D shape datasets, is able to iteratively attend to the\\nbest views targeting an object of interest for recognizing it. To realize 3D\\nview selection, we derive a 3D spatial transformer network which is\\ndifferentiable for training with backpropagation, achieving much faster\\nconvergence than the reinforcement learning employed by most existing\\nattention-based models. Experiments show that our method, with only depth\\ninput, achieves state-of-the-art next-best-view performance in time efficiency\\nand recognition accuracy.'\n",
      " b'Sliding window approaches have been widely used for object recognition tasks\\nin recent years. They guarantee an investigation of the entire input image for\\nthe object to be detected and allow a localization of that object. Despite the\\ncurrent trend towards deep neural networks, sliding window methods are still\\nused in combination with convolutional neural networks. The risk of overlooking\\nan object is clearly reduced compared to alternative detection approaches which\\ndetect objects based on shape, edges or color. Nevertheless, the sliding window\\ntechnique strongly increases the computational effort as the classifier has to\\nverify a large number of object candidates. This paper proposes a sliding\\nwindow approach which also uses depth information from a stereo camera. This\\nleads to a greatly decreased number of object candidates without significantly\\nreducing the detection accuracy. A theoretical investigation of the\\nconventional sliding window approach is presented first. Other publications to\\ndate only mentioned rough estimations of the computational cost. A mathematical\\nderivation clarifies the number of object candidates with respect to parameters\\nsuch as image and object size. Subsequently, the proposed disparity sliding\\nwindow approach is presented in detail. The approach is evaluated on pedestrian\\ndetection with annotations and images from the KITTI object detection\\nbenchmark. Furthermore, a comparison with two state-of-the-art methods is made.\\nCode is available in C++ and Python https://github.com/julimueller/\\ndisparity-sliding-window.'\n",
      " b'This paper considers the problem of designing optimal algorithms for\\nreinforcement learning in two-player zero-sum games. We focus on self-play\\nalgorithms which learn the optimal policy by playing against itself without any\\ndirect supervision. In a tabular episodic Markov game with $S$ states, $A$\\nmax-player actions and $B$ min-player actions, the best existing algorithm for\\nfinding an approximate Nash equilibrium requires $\\\\tilde{\\\\mathcal{O}}(S^2AB)$\\nsteps of game playing, when only highlighting the dependency on $(S,A,B)$. In\\ncontrast, the best existing lower bound scales as $\\\\Omega(S(A+B))$ and has a\\nsignificant gap from the upper bound. This paper closes this gap for the first\\ntime: we propose an optimistic variant of the \\\\emph{Nash Q-learning} algorithm\\nwith sample complexity $\\\\tilde{\\\\mathcal{O}}(SAB)$, and a new \\\\emph{Nash\\nV-learning} algorithm with sample complexity $\\\\tilde{\\\\mathcal{O}}(S(A+B))$. The\\nlatter result matches the information-theoretic lower bound in all\\nproblem-dependent parameters except for a polynomial factor of the length of\\neach episode. In addition, we present a computational hardness result for\\nlearning the best responses against a fixed opponent in Markov games---a\\nlearning objective different from finding the Nash equilibrium.'\n",
      " b'Tabular data is a crucial form of information expression, which can organize\\ndata in a standard structure for easy information retrieval and comparison.\\nHowever, in financial industry and many other fields tables are often disclosed\\nin unstructured digital files, e.g. Portable Document Format (PDF) and images,\\nwhich are difficult to be extracted directly. In this paper, to facilitate deep\\nlearning based table extraction from unstructured digital files, we publish a\\nstandard Chinese dataset named FinTab, which contains more than 1,600 financial\\ntables of diverse kinds and their corresponding structure representation in\\nJSON. In addition, we propose a novel graph-based convolutional neural network\\nmodel named GFTE as a baseline for future comparison. GFTE integrates image\\nfeature, position feature and textual feature together for precise edge\\nprediction and reaches overall good results.'\n",
      " b'In a traditional convolutional layer, the learned filters stay fixed after\\ntraining. In contrast, we introduce a new framework, the Dynamic Filter\\nNetwork, where filters are generated dynamically conditioned on an input. We\\nshow that this architecture is a powerful one, with increased flexibility\\nthanks to its adaptive nature, yet without an excessive increase in the number\\nof model parameters. A wide variety of filtering operations can be learned this\\nway, including local spatial transformations, but also others like selective\\n(de)blurring or adaptive feature extraction. Moreover, multiple such layers can\\nbe combined, e.g. in a recurrent architecture. We demonstrate the effectiveness\\nof the dynamic filter network on the tasks of video and stereo prediction, and\\nreach state-of-the-art performance on the moving MNIST dataset with a much\\nsmaller model. By visualizing the learned filters, we illustrate that the\\nnetwork has picked up flow information by only looking at unlabelled training\\ndata. This suggests that the network can be used to pretrain networks for\\nvarious supervised tasks in an unsupervised way, like optical flow and depth\\nestimation.'\n",
      " b'We introduce TransformerFusion, a transformer-based 3D scene reconstruction\\napproach. From an input monocular RGB video, the video frames are processed by\\na transformer network that fuses the observations into a volumetric feature\\ngrid representing the scene; this feature grid is then decoded into an implicit\\n3D scene representation. Key to our approach is the transformer architecture\\nthat enables the network to learn to attend to the most relevant image frames\\nfor each 3D location in the scene, supervised only by the scene reconstruction\\ntask. Features are fused in a coarse-to-fine fashion, storing fine-level\\nfeatures only where needed, requiring lower memory storage and enabling fusion\\nat interactive rates. The feature grid is then decoded to a higher-resolution\\nscene reconstruction, using an MLP-based surface occupancy prediction from\\ninterpolated coarse-to-fine 3D features. Our approach results in an accurate\\nsurface reconstruction, outperforming state-of-the-art multi-view stereo depth\\nestimation methods, fully-convolutional 3D reconstruction approaches, and\\napproaches using LSTM- or GRU-based recurrent networks for video sequence\\nfusion.'\n",
      " b\"The autoregressive language model (ALM) trained with maximum likelihood\\nestimation (MLE) is widely used in unconditional text generation. Due to\\nexposure bias, the generated texts still suffer from low quality and diversity.\\nThis presents statistically as a discrepancy between the real text and\\ngenerated text. Some research shows a discriminator can detect this\\ndiscrepancy. Because the discriminator can encode more information than the\\ngenerator, discriminator has the potentiality to improve generator. To\\nalleviate the exposure bias, generative adversarial networks (GAN) use the\\ndiscriminator to update the generator's parameters directly, but they fail by\\nbeing evaluated precisely. A critical reason for the failure is the difference\\nbetween the discriminator input and the ALM input. We propose a novel mechanism\\nby adding a filter which has the same input as the discriminator. First,\\ndiscriminator detects the discrepancy signals and passes to filter directly (or\\nby learning). Then, we use the filter to reject some generated samples with a\\nsampling-based method. Thus, the original generative distribution is revised to\\nreduce the discrepancy. Two ALMs, RNN-based and Transformer-based, are\\nexperimented. Evaluated precisely by three metrics, our mechanism consistently\\noutperforms the ALMs and all kinds of GANs across two benchmark data sets.\"\n",
      " b'Recent improvements to Generative Adversarial Networks (GANs) have made it\\npossible to generate realistic images in high resolution based on natural\\nlanguage descriptions such as image captions. Furthermore, conditional GANs\\nallow us to control the image generation process through labels or even natural\\nlanguage descriptions. However, fine-grained control of the image layout, i.e.\\nwhere in the image specific objects should be located, is still difficult to\\nachieve. This is especially true for images that should contain multiple\\ndistinct objects at different spatial locations. We introduce a new approach\\nwhich allows us to control the location of arbitrarily many objects within an\\nimage by adding an object pathway to both the generator and the discriminator.\\nOur approach does not need a detailed semantic layout but only bounding boxes\\nand the respective labels of the desired objects are needed. The object pathway\\nfocuses solely on the individual objects and is iteratively applied at the\\nlocations specified by the bounding boxes. The global pathway focuses on the\\nimage background and the general image layout. We perform experiments on the\\nMulti-MNIST, CLEVR, and the more complex MS-COCO data set. Our experiments show\\nthat through the use of the object pathway we can control object locations\\nwithin images and can model complex scenes with multiple objects at various\\nlocations. We further show that the object pathway focuses on the individual\\nobjects and learns features relevant for these, while the global pathway\\nfocuses on global image characteristics and the image background.']\n",
      "Label sample: [[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]]\n",
      "Text batch shape after mapping: (120,)\n",
      "Label batch shape after mapping: (120, 159)\n"
     ]
    }
   ],
   "source": [
    "# Check the structure and type of the dataset before mapping\n",
    "for text_sample, label_sample in train_dataset.take(1):\n",
    "    print(\"Text sample type:\", type(text_sample))\n",
    "    print(\"Label sample type:\", type(label_sample))\n",
    "    print(\"Text sample:\", text_sample.numpy())\n",
    "    print(\"Label sample:\", label_sample.numpy())\n",
    "\n",
    "# Apply the text_vectorizer mapping\n",
    "train_dataset = train_dataset.map(\n",
    "    lambda features, label: (features, label),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ").prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Verify the mapping by checking one batch\n",
    "for text_batch, label_batch in train_dataset.take(1):\n",
    "    print(\"Text batch shape after mapping:\", text_batch.shape)\n",
    "    print(\"Label batch shape after mapping:\", label_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e3039f-3088-4855-a82e-6c3946464618",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4cd89838-66ae-451e-b50b-0b27c3e2c669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text sample type: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "Text sample shape: (120,)\n",
      "Text sample content: b'Financial markets are a source of non-stationary multidimensional time series\\nwhich has been drawing attention for decades. Each financial instrument has its\\nspecific changing over time properties, making their analysis a complex task.\\nImprovement of understanding and development of methods for financial time\\nseries analysis is essential for successful operation on financial markets. In\\nthis study we propose a volume-based data pre-processing method for making\\nfinancial time series more suitable for machine learning pipelines. We use a\\nstatistical approach for assessing the performance of the method. Namely, we\\nformally state the hypotheses, set up associated classification tasks, compute\\neffect sizes with confidence intervals, and run statistical tests to validate\\nthe hypotheses. We additionally assess the trading performance of the proposed\\nmethod on historical data and compare it to a previously published approach.\\nOur analysis shows that the proposed volume-based method allows successful\\nclassification of the financial time series patterns, and also leads to better\\nclassification performance than a price action-based method, excelling\\nspecifically on more liquid financial instruments. Finally, we propose an\\napproach for obtaining feature interactions directly from tree-based models on\\nexample of CatBoost estimator, as well as formally assess the relatedness of\\nthe proposed approach and SHAP feature interactions with a positive outcome.'\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_dataset.take(1):\n",
    "    print(\"Text sample type:\", type(text_batch))\n",
    "    print(\"Text sample shape:\", text_batch.shape)\n",
    "    print(\"Text sample content:\", text_batch[0].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8970d479-3c4a-4704-b509-1af3e72e6693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape: (120, 159)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(\"Label shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21094c00-7224-46a3-b342-317a7e13fa97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 6/92 [>.............................] - ETA: 2:21 - loss: 0.6818 - binary_accuracy: 0.6844"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Ensure labels remain in their original shape (batch_size, 159)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m es \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m model_1\u001b[38;5;241m.\u001b[39mfit(train_dataset, validation_data\u001b[38;5;241m=\u001b[39mval_dataset, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[es])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\u001b[38;5;241m.\u001b[39m_call_flat(\n\u001b[0;32m    144\u001b[0m     filtered_flat_args, captured_inputs\u001b[38;5;241m=\u001b[39mconcrete_function\u001b[38;5;241m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall(\n\u001b[0;32m   1758\u001b[0m       ctx, args, cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager))\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    383\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m    384\u001b[0m         inputs\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m    385\u001b[0m         attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m    386\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx)\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Vectorize text input\n",
    "text_vectorizer = layers.TextVectorization(max_tokens=20000, ngrams=2, output_mode='tf_idf')\n",
    "text_vectorizer.adapt(train_dataset.map(lambda x, y: x))  # Adapt to the training dataset\n",
    "\n",
    "vocab_size = len(text_vectorizer.get_vocabulary())\n",
    "embedding_dim = 128  # Choose an appropriate embedding dimension\n",
    "\n",
    "# Ensure model architecture matches input shape expectations\n",
    "model_1 = models.Sequential([\n",
    "    text_vectorizer,  # Preprocess text input\n",
    "    layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=None),\n",
    "    layers.GlobalAveragePooling1D(),  # Convert 2D tensor to 1D\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(159, activation='sigmoid')  # Adjust output layer for multi-label classification\n",
    "])\n",
    "\n",
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])\n",
    "\n",
    "# Ensure labels remain in their original shape (batch_size, 159)\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "history = model_1.fit(train_dataset, validation_data=val_dataset, epochs=20, callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e1dad87-2352-4f12-a21b-9dc8961eb2e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid()\n\u001b[0;32m     12\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 15\u001b[0m plot_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m plot_result(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m, in \u001b[0;36mplot_result\u001b[1;34m(item)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_result\u001b[39m(item):\n\u001b[1;32m----> 5\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[item], label\u001b[38;5;241m=\u001b[39mitem)\n\u001b[0;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m item], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m item)\n\u001b[0;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "#model_1.save('Models/model_1', save_format='tf')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_result(item):\n",
    "    plt.plot(history.history[item], label=item)\n",
    "    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(item)\n",
    "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_result(\"loss\")\n",
    "plot_result(\"binary_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0de29d70-1b2f-4b62-b01d-92d17c7a6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "saved_text_vectorizer_config = text_vectorizer.get_config()\n",
    "with open('Models/text_vectorizer_config.pkl' , 'wb') as f:\n",
    "    pickle.dump(saved_text_vectorizer_config , f)\n",
    "\n",
    "with open('Models/vocab.pkl','wb') as f:\n",
    "    pickle.dump(vocab , f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06ec1d8d-c3ef-419f-bc5b-152ba4381b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration and weights saved successfully\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Assuming `text_vectorizer` is already adapted\n",
    "text_vectorizer = TextVectorization()\n",
    "text_vectorizer.adapt(train_dataset.map(lambda x, y: x))\n",
    "\n",
    "# Save the configuration\n",
    "with open('Models/text_vectorizer_config.pkl', 'wb') as f:\n",
    "    pickle.dump(text_vectorizer.get_config(), f)\n",
    "\n",
    "# Save the weights (vocabulary)\n",
    "with open('Models/text_vectorizer_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(text_vectorizer.get_vocabulary(), f)\n",
    "\n",
    "print(\"Configuration and weights saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6ed4158-7a0e-4190-b652-c1f4226b4158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextVectorization layer loaded successfully with the vocabulary\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Load the configuration\n",
    "with open('Models/text_vectorizer_config.pkl', 'rb') as f:\n",
    "    loaded_config = pickle.load(f)\n",
    "\n",
    "# Create a new TextVectorization layer from the config\n",
    "loaded_text_vectorizer = TextVectorization.from_config(loaded_config)\n",
    "\n",
    "# Load the vocabulary and set it in the new layer\n",
    "with open('Models/text_vectorizer_weights.pkl', 'rb') as f:\n",
    "    loaded_vocab = pickle.load(f)\n",
    "    loaded_text_vectorizer.set_vocabulary(loaded_vocab)\n",
    "\n",
    "print(\"TextVectorization layer loaded successfully with the vocabulary\")\n",
    "loaded_model = keras.models.load_model(\"Models/model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dfa9d3e-faf2-468c-bdb2-28db9eeb69ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 61s 651ms/step - loss: 0.6444 - binary_accuracy: 0.9380\n",
      "6/6 [==============================] - 3s 553ms/step - loss: 0.6444 - binary_accuracy: 0.9380\n"
     ]
    }
   ],
   "source": [
    "_ , acc1 = model_1.evaluate(test_dataset)\n",
    "_ , acc1 = model_1.evaluate(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8a050b-8dfa-4234-a1d7-5693ac1a4139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 - Shape of abstract: (1,)\n",
      "Step 2 - Shape of preprocessed_abstract: (1,)\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "Shape of predictions: (1, 159)\n",
      "Predicted Categories: ['the' 'of']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def invert_multi_hot(encoded_labels):\n",
    "    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n",
    "    try:\n",
    "        hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n",
    "        if hot_indices.size == 0:\n",
    "            print(\"Warning: No active indices found in encoded labels.\")\n",
    "        return np.take(loaded_vocab, hot_indices)\n",
    "    except IndexError as e:\n",
    "        print(f\"Index error during label lookup: {e}\")\n",
    "        return ['[UNK]']  # Return a placeholder for unexpected issues.\n",
    "\n",
    "def predict_category(abstract, model, vectorizer, label_lookup):\n",
    "    try:\n",
    "        # Ensure the input is a list containing the abstract string\n",
    "        preprocessed_abstract = [abstract]\n",
    "        print(\"Step 1 - Shape of abstract:\", np.shape(preprocessed_abstract))\n",
    "\n",
    "        # Pass the input directly to the model without calling vectorizer outside of it\n",
    "        preprocessed_abstract = tf.convert_to_tensor(preprocessed_abstract)\n",
    "        print(\"Step 2 - Shape of preprocessed_abstract:\", preprocessed_abstract.shape)\n",
    "\n",
    "        # Make predictions using the loaded model (model should handle vectorization)\n",
    "        predictions = loaded_model.predict(preprocessed_abstract)\n",
    "        print(\"Shape of predictions:\", predictions.shape)\n",
    "\n",
    "        # Convert predictions to human-readable labels\n",
    "        predicted_labels = label_lookup(np.round(predictions).astype(int)[0])\n",
    "\n",
    "        return predicted_labels\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "new_abstract = \"Graph neural networks (GNNs) have been widely used to learn vector representation of graph-structured data and achieved better task performance than conventional methods...\"\n",
    "predicted_categories = predict_category(new_abstract, model_1, text_vectorizer, invert_multi_hot)\n",
    "print(\"Predicted Categories:\", predicted_categories)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228b5e0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eebe3056-da83-4d3e-98e8-a5c229e2cf39",
   "metadata": {},
   "source": [
    "# LLM Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8f736a6-c859-4e95-b9d5-05a8e16a8449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (3.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.46.1)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.2.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.23.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "156cc5bd-42b9-4b2e-b449-1f1e4a5ade3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sentence-transformers\n",
      "Version: 3.2.1\n",
      "Summary: State-of-the-Art Text Embeddings\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Nils Reimers <info@nils-reimers.de>, Tom Aarsen <tom.aarsen@huggingface.co>\n",
      "License: Apache 2.0\n",
      "Location: C:\\Users\\VARSHITH\\AppData\\Roaming\\Python\\Python311\\site-packages\n",
      "Requires: huggingface-hub, Pillow, scikit-learn, scipy, torch, tqdm, transformers\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61766a39-e52f-4b5c-bf2b-8a3c86e49a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jupyterlab in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (4.3.0)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (8.1.5)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from jupyterlab) (0.27.2)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (6.28.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (3.1.3)\n",
      "Requirement already satisfied: jupyter-core in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (5.5.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (2.2.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (2.10.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from jupyterlab) (2.27.3)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (0.2.3)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (23.1)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (68.2.2)\n",
      "Requirement already satisfied: tornado>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (6.3.3)\n",
      "Requirement already satisfied: traitlets in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab) (5.7.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets) (8.20.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from httpx>=0.25.0->jupyterlab) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (3.4)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx>=0.25.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab) (0.14.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (1.6.7)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (8.6.0)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (1.6.0)\n",
      "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipykernel>=6.5.0->jupyterlab) (25.1.2)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2>=3.0.3->jupyterlab) (2.1.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterlab) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-core->jupyterlab) (305.1)\n",
      "Requirement already satisfied: argon2-cffi in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (21.3.0)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.8.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.10.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (5.9.2)\n",
      "Requirement already satisfied: overrides in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.14.1)\n",
      "Requirement already satisfied: pywinpty in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (2.0.10)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-server<3,>=2.4.0->jupyterlab) (0.58.0)\n",
      "Requirement already satisfied: babel>=2.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (4.19.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab) (2.31.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from babel>=2.10->jupyterlab-server<3,>=2.27.1->jupyterlab) (2023.3.post1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel>=6.5.0->jupyterlab) (2.8.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in c:\\programdata\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in c:\\programdata\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.16.2)\n",
      "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->jupyterlab) (2.0.7)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\programdata\\anaconda3\\lib\\site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (21.2.0)\n",
      "Requirement already satisfied: executing in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from websocket-client->jupyter-server<3,>=2.4.0->jupyterlab) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\programdata\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (0.5.1)\n",
      "Requirement already satisfied: fqdn in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (2.1)\n",
      "Requirement already satisfied: uri-template in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in c:\\users\\varshith\\appdata\\roaming\\python\\python311\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (24.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->jupyterlab) (2.5)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->jupyterlab) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->jupyterlab) (1.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade jupyterlab ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43fb7bc1-04a2-47d8-b004-349c415e5978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VARSHITH\\AppData\\Roaming\\Python\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer , util\n",
    "from tqdm.autonotebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6602bf8-c0aa-4d50-8aaa-58b77fc43a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90f3d369-12b5-472e-b890-cb15fda10075",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76c3b692-e9e7-493f-b5e9-a6a66319763c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = arxiv_data['titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d11803f-edbf-41de-907c-81f2902fd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3eb0979b-91bd-4f8b-8fc2-276a897c5d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 384)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afb54d09-98eb-4a0e-8f8a-913b2de42629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence :  Multi-Level Attention Pooling for Graph Neural Networks: Unifying Graph Representations with Multiple Localities\n",
      "Embeddings :  384\n",
      "Sentence :  Decision Forests vs. Deep Networks: Conceptual Similarities and Empirical Differences at Small Sample Sizes\n",
      "Embeddings :  384\n",
      "Sentence :  Power up! Robust Graph Convolutional Network via Graph Powering\n",
      "Embeddings :  384\n",
      "Sentence :  Releasing Graph Neural Networks with Differential Privacy Guarantees\n",
      "Embeddings :  384\n",
      "Sentence :  Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
      "Embeddings :  384\n",
      "Sentence :  Lifelong Graph Learning\n",
      "Embeddings :  384\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "\n",
    "for sentence , embedding in zip(sentences , embeddings):\n",
    "    print(\"Sentence : \" , sentence)\n",
    "    print(\"Embeddings : \" , len(embedding))\n",
    "    c = c + 1\n",
    "    if c > 5:\n",
    "        break;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e51875b-1aa8-488d-b087-763869d5aa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"embeddings.pkl\" , 'wb') as f:\n",
    "    pickle.dump(embeddings , f)\n",
    "\n",
    "with open(\"sentences.pkl\" , 'wb') as f:\n",
    "    pickle.dump(sentences , f)\n",
    "\n",
    "with open(\"Trained_model.pkl\" , 'wb') as f:\n",
    "    pickle.dump(model , f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b103c42-7e8c-4a76-bcde-36bc61c301d0",
   "metadata": {},
   "source": [
    "# Reccomendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb3ba6cb-b1e6-4e34-b357-ff5fea953ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "embeddings = pickle.load(open(\"embeddings.pkl\" , 'rb'))\n",
    "tences = pickle.load(open(\"sentences.pkl\" , 'rb'))\n",
    "rec_model = pickle.load(open(\"Trained_model.pkl\" , 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac574c8c-2093-482f-8bab-6a7de175fe71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Multi-Level Attention Pooling for Graph Neural...\n",
       "1        Decision Forests vs. Deep Networks: Conceptual...\n",
       "2        Power up! Robust Graph Convolutional Network v...\n",
       "3        Releasing Graph Neural Networks with Different...\n",
       "4        Recurrence-Aware Long-Term Cognitive Network f...\n",
       "                               ...                        \n",
       "56176    Mining Spatio-temporal Data on Industrializati...\n",
       "56177    Wav2Letter: an End-to-End ConvNet-based Speech...\n",
       "56178    Deep Reinforcement Learning with Double Q-lear...\n",
       "56179                          Generalized Low Rank Models\n",
       "56180    Chi-square Tests Driven Method for Learning th...\n",
       "Name: titles, Length: 56181, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def recommendation(input_paper):\n",
    "    input_embedding = rec_model.encode(input_paper)\n",
    "    cosine_scores = util.cos_sim(embeddings , input_embedding)\n",
    "    #k = min(5, cosine_scores.size(0))\n",
    "    top_similar_papers = torch.topk(cosine_scores , dim = 0 , k = 8 , sorted = True)\n",
    "\n",
    "    paper_list = set()\n",
    "    for i in top_similar_papers.indices:\n",
    "        paper_list.add(tences[i.item()])\n",
    "\n",
    "    return paper_list\n",
    "\n",
    "tences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9625393b-1926-4dea-829c-73c318620275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the title of the Research paper Lifelong Graph Learning\n"
     ]
    }
   ],
   "source": [
    "input_paper = input(\"Enter the title of the Research paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd5419bb-fa33-4ea3-ade3-e2b2d21fe92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We recommend to read this paper ................ \n",
      "\n",
      "An Uncoupled Training Architecture for Large Graph Learning\n",
      "Graph Learning with Loss-Guided Training\n",
      "Graph-Based Continual Learning\n",
      "Lifelong Graph Learning\n",
      "Lifelong Learning of Graph Neural Networks for Open-World Node Classification\n"
     ]
    }
   ],
   "source": [
    "recommend_paper = recommendation(input_paper)\n",
    "print(\"We recommend to read this paper ................ \\n\")\n",
    "for paper in sorted(recommend_paper):\n",
    "    print(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6f594-de19-4fe6-b649-53a504fa7362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
